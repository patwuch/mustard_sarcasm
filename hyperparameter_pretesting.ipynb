{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chwu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chwu/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import itertools\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset, random_split\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#We load in the previously extracted and saved features (last four vs final)\n",
    "import pickle\n",
    "with open('/home/chwu/nonlayered_mustard_updated_text.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.dropna(subset=['text_embeddings']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.rename(columns={\n",
    "    'text_embeddings': 'uText',\n",
    "    'audio_embeddings': 'uAudio',\n",
    "    'keyframe_embeddings': 'uVideo',\n",
    "    'Sarcasm':'SAR'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cAudio'] = None\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cAudio'] = new_df.at[i, 'uAudio']\n",
    "\n",
    "new_df['cVideo'] = None\n",
    "\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cVideo'] = new_df.at[i, 'uVideo']\n",
    "\n",
    "new_df['cText'] = None\n",
    "\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cText'] = new_df.at[i, 'uText']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna(subset=['cAudio']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select a 450 video sample to conduct hyperparameter search\n",
    "hyp_pos = new_df[new_df['SAR']==1].sample(225).reset_index()\n",
    "hyp_neg = new_df[new_df['SAR']==0].sample(225).reset_index()\n",
    "h_train = pd.concat([hyp_pos[75:], hyp_neg[75:]], ignore_index=True)\n",
    "h_val = pd.concat([hyp_pos[:74], hyp_neg[:74]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we map the train and validation set for hyperparameter search into appropriate formats so the Custom ContentDataset class can process them properly.\n",
    "dataset1 = {}\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "dataset1 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in h_train.iterrows()\n",
    "}\n",
    "\n",
    "dataset2 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in h_val.iterrows()\n",
    "}\n",
    "\n",
    "map1 = h_train[['SCENE','SAR','SPEAKER']]\n",
    "map2 = h_val[['SCENE','SAR','SPEAKER']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load in the speaker list from the full dataset\n",
    "speaker_list = sorted(list(df.SPEAKER.value_counts().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_attention(tensor):\n",
    "    # Compute attention scores\n",
    "        attention_layer = torch.nn.Linear(768, 1)\n",
    "        attention_scores = attention_layer(tensor)  # [batch_size, seq_len, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len, 1]\n",
    "    \n",
    "    # Weighted sum of the input tensor\n",
    "        attended_tensor = (tensor * attention_weights).sum(dim=-2)  # [batch_size, hidden_dim]\n",
    "    \n",
    "        return attended_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the custom Content Dataset class used to load in extracted features, speaker info, and sarcasm label.\n",
    "#this is adapted from MUStARD++'s code\n",
    "#commented out section is the version used for mean of last four layer\n",
    "class ContentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mapping, dataset, speaker_list):\n",
    "        self.mapping = mapping\n",
    "        self.dataset = dataset\n",
    "        self.speakers_mapping = speaker_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        index = self.mapping.loc[idx, 'SCENE']\n",
    "        data = self.dataset[index]\n",
    "        label = int(self.mapping.loc[idx, 'SAR'])\n",
    "        spkr = np.eye(len(self.speakers_mapping))[self.speakers_mapping.index(\n",
    "            self.mapping.loc[idx, 'SPEAKER'])]\n",
    "        uText = data['uText'].squeeze()\n",
    "        cText = data['cText'].squeeze()\n",
    "        uAudio = apply_attention(data['uAudio']).squeeze()\n",
    "        cAudio = apply_attention(data['cAudio']).squeeze()\n",
    "        uVideo = apply_attention(data['uVideo']).squeeze()\n",
    "        cVideo = apply_attention(data['cVideo']).squeeze()\n",
    "\n",
    "        return uText, cText, uAudio, cAudio, uVideo, cVideo, spkr, label\n",
    "\n",
    "# class ContentDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, mapping, dataset, speaker_list):\n",
    "#         self.mapping = mapping\n",
    "#         self.dataset = dataset\n",
    "#         self.speakers_mapping = speaker_list\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.mapping)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         index = self.mapping.loc[idx, 'SCENE']\n",
    "#         data = self.dataset[index]\n",
    "#         label = int(self.mapping.loc[idx, 'SAR'])\n",
    "#         spkr = np.eye(len(self.speakers_mapping))[self.speakers_mapping.index(\n",
    "#             self.mapping.loc[idx, 'SPEAKER'])]\n",
    "#         uText = data['uText'].squeeze()\n",
    "#         cText = data['cText'].squeeze()\n",
    "#         uAudio = data['uAudio'].squeeze()\n",
    "#         cAudio = data['cAudio'].squeeze()\n",
    "#         uVideo = data['uVideo'].squeeze()\n",
    "#         cVideo = data['cVideo'].squeeze()\n",
    "\n",
    "#         return uText, cText, uAudio, cAudio, uVideo, cVideo, spkr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the features from the datafram into the ContentDataset class with its respective fold\n",
    "CD1 = ContentDataset(mapping=map1, dataset=dataset1,speaker_list=speaker_list)\n",
    "CD2 = ContentDataset(mapping=map2, dataset=dataset2,speaker_list=speaker_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uText: torch.Size([768])\n",
      "cText: torch.Size([768])\n",
      "uAudio: torch.Size([768])\n",
      "cAudio: torch.Size([768])\n",
      "uVideo: torch.Size([768])\n",
      "cVideo: torch.Size([768])\n",
      "spkr: (33,)\n",
      "label: 0\n"
     ]
    }
   ],
   "source": [
    "#we inspect whether a sample of the ContentDataset class is in the right shape\n",
    "index = 9  # Example index\n",
    "entry = CD2[index]\n",
    "\n",
    "# Print the entry\n",
    "print(\"uText:\", entry[0].shape)\n",
    "print(\"cText:\", entry[1].shape)\n",
    "print(\"uAudio:\", entry[2].shape)\n",
    "print(\"cAudio:\", entry[3].shape)\n",
    "print(\"uVideo:\", entry[4].shape)\n",
    "print(\"cVideo:\", entry[5].shape)\n",
    "print(\"spkr:\", entry[6].shape)\n",
    "print(\"label:\", entry[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks (seed, evaluation, training, get_command, get_models_and_parameters, seed_worker) is largely replicated from MUStARD++'s training and evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed():\n",
    "    \"\"\" This method is used for seeding the code and different points\"\"\"\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(loader, mod, call, report=False, flag=False):\n",
    "    \"\"\"Args:\n",
    "            loader:\n",
    "                It is the validation dataloader\n",
    "            mod:\n",
    "                It is the best model, which we have to evaluate\n",
    "            call:\n",
    "                call is the COMMAND to be excuted to run the forward method of the model\n",
    "                it changed as per the modality and other possible input\n",
    "            report:\n",
    "                If True then the classification report for the validation set is printed\n",
    "            flag:\n",
    "                if True the instead of evaluation metrics, method returns the class labels (predictions)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = []\n",
    "        true = []\n",
    "        total_loss = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion.to(device)\n",
    "        seed()\n",
    "        for batch in loader:\n",
    "            uText = batch[0].float().to(device)\n",
    "            cText = batch[1].float().to(device)\n",
    "            uAudio = batch[2].float().to(device)\n",
    "            cAudio = batch[3].float().to(device)\n",
    "            uVideo = batch[4].float().to(device)\n",
    "            cVideo = batch[5].float().to(device)\n",
    "            speaker = batch[6].float().to(device)\n",
    "            y_true = batch[7].long().to(device)\n",
    "            del batch\n",
    "            output = torch.softmax(eval(call), dim=1)\n",
    "            loss = criterion(output, y_true)\n",
    "            del uText, cText, uAudio, cAudio, uVideo, cVideo, speaker\n",
    "            # with torch.cuda.device(device):\n",
    "            #     torch.cuda.empty_cache()\n",
    "            total_loss.append(loss)\n",
    "            pred.extend(output.detach().cpu().tolist())\n",
    "            true.extend(y_true.tolist())\n",
    "        if flag:\n",
    "            return true, np.argmax(pred, axis=1)\n",
    "        if report:\n",
    "            print(classification_report(true, np.argmax(pred, axis=1), digits=3))\n",
    "\n",
    "        \n",
    "        return f1_score(true, np.argmax(pred, axis=1), average='macro'), sum(total_loss)/len(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(mod, criterion, optimizer, call, train_loader, valid_loader, fold, e=500, patience=5, report=False,save=True):\n",
    "    \"\"\"Args:\n",
    "            mod :\n",
    "                It is the mod we have to train\n",
    "            criterion :\n",
    "                Loss function, here we have Cross entropy loss\n",
    "            optimizer :\n",
    "              object of torch.optim class\n",
    "            call:\n",
    "                call is the COMMAND to be excuted to run the forward method of the model\n",
    "                it changed as per the modality and other possible input\n",
    "            train_loader:\n",
    "                It is a instance of train dataloader\n",
    "            valid_loader:\n",
    "                It is a instance of validation dataloader, it is given as a input to evaluation class\n",
    "            fold:\n",
    "                5 FOLD {0,1,2,3,4}\n",
    "            e:\n",
    "                maximum epoch\n",
    "            patience:\n",
    "                how many epoch to wait after the early stopping condition in satisfied\n",
    "            report:\n",
    "                It True then the classification report for the validation set is printed, it is given as a input to evaluation class\n",
    "            save:\n",
    "                If true then best model for each fold is saved\n",
    "\n",
    "    \"\"\"\n",
    "    print('-'*100)\n",
    "    train_losses = [0]\n",
    "    valid_losses = [0]\n",
    "    max_f1 = 0\n",
    "    patience_flag = 1\n",
    "    best_epoch = 0\n",
    "    print(fold, e, patience)\n",
    "\n",
    "    while e > 0:\n",
    "        total_loss = []\n",
    "        seed()\n",
    "        for batch_data in train_loader:\n",
    "            uText = batch_data[0].float().to(device)\n",
    "            cText = batch_data[1].float().to(device)\n",
    "            uAudio = batch_data[2].float().to(device)\n",
    "            cAudio = batch_data[3].float().to(device)\n",
    "            uVideo = batch_data[4].float().to(device)\n",
    "            cVideo = batch_data[5].float().to(device)\n",
    "            speaker = batch_data[6].float().to(device)\n",
    "            y_true = batch_data[7].long().to(device)\n",
    "            del batch_data\n",
    "            output = eval(call)\n",
    "            loss = criterion(output, y_true)\n",
    "            del uText, cText, uAudio, cAudio, uVideo, cVideo, speaker\n",
    "            # with torch.cuda.device(device):\n",
    "            #     torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.append(loss.detach().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            valid_f1, valid_loss = evaluation(\n",
    "                valid_loader, mod, call, report, False)\n",
    "            train_losses.append(sum(total_loss)/len(total_loss))\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            e = e-1\n",
    "            if max_f1 < valid_f1:\n",
    "                max_f1 = valid_f1\n",
    "                best_model = mod\n",
    "                best_epoch = 500-e\n",
    "                print(\n",
    "                    f'Epoch:{best_epoch} | Train Loss: {loss.detach().item():.3f} | Valid loss: { valid_loss.detach().item():7.3f} | Valid F1: { valid_f1:7.3f}')\n",
    "\n",
    "            if abs(train_losses[-2]-train_losses[-1]) < 0.0001:\n",
    "                if patience_flag == 1:\n",
    "                    e = patience\n",
    "                    patience_flag = 0\n",
    "            else:\n",
    "                patience_flag = 1\n",
    "\n",
    "    # if save:\n",
    "    #     best_model.to(device)\n",
    "    #     torch.save(best_model.state_dict(), 'MPP_Code/saved_models/sarc/' +\n",
    "    #                         filename+'_'+str(fold)+'.pth')\n",
    "                \n",
    "    return evaluation(valid_loader, best_model, call, report, True), best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_command(input_modes, context_flag, speaker_flag):\n",
    "    \"\"\"\n",
    "        This method is used to create the COMMAND to execute the forward methof of particular model,\n",
    "        Depending upon the input combination\n",
    "        Args:\n",
    "            input_modes:\n",
    "                Input Modality {VTA, VT, VA, TA, V, T, A}\n",
    "            context_flag :\n",
    "                If true then \"with context\" else \"without context\" \n",
    "            speaker_flag:\n",
    "                if true then Speaker dependent else Speaker INdependent\n",
    "    \"\"\"\n",
    "    if input_modes == 'VTA':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uText, 'uC':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cText, 'cC':cAudio\"\n",
    "\n",
    "    elif input_modes == 'VT':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uText\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cText\"\n",
    "\n",
    "    elif input_modes == 'VA':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cAudio\"\n",
    "\n",
    "    elif input_modes == 'TA':\n",
    "        COMMAND = \"mod(**{'uA':uText, 'uB':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cText, 'cB':cAudio\"\n",
    "\n",
    "    elif input_modes == 'T':\n",
    "        COMMAND = \"mod(**{'uA':uText\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cText\"\n",
    "\n",
    "    elif input_modes == 'V':\n",
    "        COMMAND = \"mod(**{'uA':uVideo\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo\"\n",
    "\n",
    "    elif input_modes == 'A':\n",
    "        COMMAND = \"mod(**{'uA':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cAudio\"\n",
    "    if speaker_flag == 'y':\n",
    "        COMMAND += \",'speaker_embedding':speaker})\"\n",
    "    else:\n",
    "        COMMAND += \"})\"\n",
    "\n",
    "    return COMMAND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes has been changed to 2 as we are performing binary sarcasm detection\n",
    "#all tensors shape have been changed to 768 to correspond to the tensor shape of Data2Vec after reshaping with ContentDataset\n",
    "def get_model_and_parameters(mode, speaker, context):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - mode: A string representing the mode.\n",
    "    - speaker: A string ('y' or 'n') indicating whether the speaker is dependent or not.\n",
    "    - context: A string ('y' or 'n') indicating whether context is considered or not.\n",
    "    \"\"\"\n",
    "    # Here we are sorting VTA in descending order, in order to have consistency in the model\n",
    "    input_modes = ''.join(reversed(sorted(list(mode.upper()))))\n",
    "\n",
    "    parameters = {}\n",
    "    MODEL_NAME = 'Speaker_'\n",
    "\n",
    "    parameters['num_classes'] = 2\n",
    "\n",
    "    if speaker.lower() == 'y':\n",
    "        MODEL_NAME += 'Dependent_'\n",
    "        parameters['n_speaker'] = len(speaker_list)  # Make sure speaker_list is defined elsewhere in your code\n",
    "    else:\n",
    "        MODEL_NAME += 'Independent_'\n",
    "\n",
    "    if len(input_modes) == 3:\n",
    "        MODEL_NAME += 'Triple_'\n",
    "        parameters['input_embedding_A'] = 768\n",
    "        parameters['input_embedding_B'] = 768\n",
    "        parameters['input_embedding_C'] = 768\n",
    "\n",
    "    elif len(input_modes) == 2:\n",
    "        MODEL_NAME += 'Dual_'\n",
    "        parameters['input_embedding_A'] = 768\n",
    "        parameters['input_embedding_B'] = 768 \n",
    "    else:\n",
    "        MODEL_NAME += 'Single_'\n",
    "        parameters['input_embedding_A'] = 768 \n",
    "\n",
    "    MODEL_NAME += 'Mode_with'\n",
    "    MODEL_NAME += 'out' if context.lower() == 'n' else ''\n",
    "    MODEL_NAME += '_Context'\n",
    "\n",
    "    MODEL_NAME = 'emotion_classification_model.' + MODEL_NAME\n",
    "\n",
    "    COMMAND = get_command(input_modes, context.lower(), speaker.lower())  # Ensure get_command is defined\n",
    "    return MODEL_NAME, parameters, COMMAND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    \"\"\" This method is used for seeding the worker in the dataloader\"\"\"\n",
    "    worker_seed = 42\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here only one model is included. Models of different ablation setting are identical to that of MUStARD++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Speaker_Dependent_Triple_Mode_with_Context(nn.Module):\n",
    "    def __init__(self, n_speaker=len(speaker_list), input_embedding_A=768, input_embedding_B=768, input_embedding_C=768, shared_embedding =768, projection_embedding=768, dropout=0.5, num_classes=2):\n",
    "        super(Speaker_Dependent_Triple_Mode_with_Context, self).__init__()\n",
    "\n",
    "        self.n_speaker = n_speaker\n",
    "\n",
    "        self.input_embedding_A = input_embedding_A\n",
    "        self.input_embedding_B = input_embedding_B\n",
    "        self.input_embedding_C = input_embedding_C\n",
    "\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.projection_embedding = projection_embedding\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.A_context_share = nn.Linear(self.input_embedding_A, self.shared_embedding)\n",
    "        self.A_utterance_share = nn.Linear(self.input_embedding_A, self.shared_embedding)\n",
    "\n",
    "        self.C_context_share = nn.Linear(self.input_embedding_C, self.shared_embedding)\n",
    "        self.C_utterance_share = nn.Linear(self.input_embedding_C, self.shared_embedding)\n",
    "\n",
    "        self.B_context_share = nn.Linear(self.input_embedding_B, self.shared_embedding)\n",
    "        self.B_utterance_share = nn.Linear(self.input_embedding_B, self.shared_embedding)\n",
    "\n",
    "        self.norm_A_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_A_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.norm_C_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_C_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.norm_B_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_B_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.collaborative_gate_1 = nn.Linear(2 * self.shared_embedding, self.projection_embedding)\n",
    "        self.collaborative_gate_2 = nn.Linear(self.projection_embedding, self.shared_embedding)\n",
    "\n",
    "        self.pred_module = nn.Sequential(\n",
    "            nn.Linear(self.n_speaker + 3 * self.shared_embedding, 2 * self.shared_embedding),\n",
    "            nn.BatchNorm1d(2 * self.shared_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2 * self.shared_embedding, self.shared_embedding),\n",
    "            nn.BatchNorm1d(self.shared_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.shared_embedding, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def attention(self, featureA, featureB):\n",
    "        \"\"\" This method takes two features and calculates the attention \"\"\"\n",
    "        input = torch.cat((featureA, featureB), dim=1)\n",
    "        return nn.functional.softmax(self.collaborative_gate_1(input), dim=1)\n",
    "    \n",
    "\n",
    "    def attention_aggregator(self, feA, feB, feC, feD, feE, feF):\n",
    "        \"\"\" This method calculates the attention for feA with respect to others\"\"\"\n",
    "        input = self.attention(feA, feB) + self.attention(feA, feC) + self.attention(feA, feD) + self.attention(feA, feE) + self.attention(feA, feF)\n",
    "        return nn.functional.softmax(self.collaborative_gate_2(input), dim=1)\n",
    "\n",
    "    def forward(self, uA, cA, uB, cB, uC, cC, speaker_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            uA: Utterance Video\n",
    "            uB: Utterance Text\n",
    "            uC: Utterance Audio\n",
    "            cA: Context Video\n",
    "            cB: Context Text\n",
    "            cC: Context Audio\n",
    "\n",
    "        Returns:\n",
    "            probability of emotion classes\n",
    "        \"\"\"\n",
    "        # Pooling or averaging the sequences to match the expected input dimensions for linear layers\n",
    "        \n",
    "        \n",
    "        shared_A_context = self.norm_A_context(nn.functional.relu(self.A_context_share(cA)))\n",
    "        shared_A_utterance = self.norm_A_utterance(nn.functional.relu(self.A_utterance_share(uA)))\n",
    "\n",
    "        shared_C_context = self.norm_C_context(nn.functional.relu(self.C_context_share(cC)))\n",
    "        shared_C_utterance = self.norm_C_utterance(nn.functional.relu(self.C_utterance_share(uC)))\n",
    "\n",
    "        shared_B_context = self.norm_B_context(nn.functional.relu(self.B_context_share(cB)))\n",
    "        shared_B_utterance = self.norm_B_utterance(nn.functional.relu(self.B_utterance_share(uB)))\n",
    "\n",
    "        updated_shared_A = shared_A_utterance * self.attention_aggregator(\n",
    "            shared_A_utterance, shared_A_context, shared_C_context, shared_C_utterance, shared_B_context, shared_B_utterance)\n",
    "        updated_shared_C = shared_C_utterance * self.attention_aggregator(\n",
    "            shared_C_utterance, shared_C_context, shared_A_context, shared_A_utterance, shared_B_context, shared_B_utterance)\n",
    "        updated_shared_B = shared_B_utterance * self.attention_aggregator(\n",
    "            shared_B_utterance, shared_B_context, shared_A_context, shared_A_utterance, shared_C_context, shared_C_utterance)\n",
    "\n",
    "        temp = torch.cat((updated_shared_A, updated_shared_C), dim=1)\n",
    "        input = torch.cat((temp, updated_shared_B), dim=1)\n",
    "\n",
    "        input = torch.cat((input, speaker_embedding), dim=1)\n",
    "\n",
    "        return self.pred_module(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with dropout=0.2, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.631 | Valid loss:   0.677 | Valid F1:   0.558\n",
      "Epoch:2 | Train Loss: 0.422 | Valid loss:   0.678 | Valid F1:   0.576\n",
      "Epoch:3 | Train Loss: 0.150 | Valid loss:   0.689 | Valid F1:   0.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.634     0.656     0.645        90\n",
      "           1      0.644     0.622     0.633        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "New best params: (0.2, 0.001, 16, 2048, 1024) with loss: 0.6468186974525452 and F1 score of :0.63878855237566\n",
      "Training with dropout=0.2, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.673 | Valid loss:   0.677 | Valid F1:   0.565\n",
      "Epoch:2 | Train Loss: 0.312 | Valid loss:   0.680 | Valid F1:   0.593\n",
      "Epoch:3 | Train Loss: 0.148 | Valid loss:   0.688 | Valid F1:   0.593\n",
      "Epoch:6 | Train Loss: 0.236 | Valid loss:   0.693 | Valid F1:   0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.626     0.633     0.630        90\n",
      "           1      0.629     0.622     0.626        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.628       180\n",
      "weighted avg      0.628     0.628     0.628       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.604 | Valid loss:   0.667 | Valid F1:   0.580\n",
      "Epoch:2 | Train Loss: 0.263 | Valid loss:   0.678 | Valid F1:   0.593\n",
      "Epoch:6 | Train Loss: 0.047 | Valid loss:   0.701 | Valid F1:   0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.637     0.644     0.641        90\n",
      "           1      0.640     0.633     0.637        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.501 | Valid loss:   0.657 | Valid F1:   0.594\n",
      "Epoch:2 | Train Loss: 0.278 | Valid loss:   0.656 | Valid F1:   0.620\n",
      "Epoch:8 | Train Loss: 0.035 | Valid loss:   0.643 | Valid F1:   0.628\n",
      "Epoch:9 | Train Loss: 0.081 | Valid loss:   0.658 | Valid F1:   0.644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.645     0.667     0.656        90\n",
      "           1      0.655     0.633     0.644        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "New best params: (0.2, 0.001, 16, 1024, 256) with loss: 0.6257906556129456 and F1 score of :0.6499027507641011\n",
      "Training with dropout=0.2, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.791 | Valid loss:   0.668 | Valid F1:   0.561\n",
      "Epoch:2 | Train Loss: 0.303 | Valid loss:   0.664 | Valid F1:   0.622\n",
      "Epoch:3 | Train Loss: 0.096 | Valid loss:   0.670 | Valid F1:   0.628\n",
      "Epoch:8 | Train Loss: 0.010 | Valid loss:   0.643 | Valid F1:   0.655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.656     0.659        90\n",
      "           1      0.659     0.667     0.663        90\n",
      "\n",
      "    accuracy                          0.661       180\n",
      "   macro avg      0.661     0.661     0.661       180\n",
      "weighted avg      0.661     0.661     0.661       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.758 | Valid loss:   0.663 | Valid F1:   0.563\n",
      "Epoch:2 | Train Loss: 0.243 | Valid loss:   0.665 | Valid F1:   0.600\n",
      "Epoch:3 | Train Loss: 0.063 | Valid loss:   0.667 | Valid F1:   0.622\n",
      "Epoch:7 | Train Loss: 0.029 | Valid loss:   0.666 | Valid F1:   0.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.655     0.611     0.632        90\n",
      "           1      0.635     0.678     0.656        90\n",
      "\n",
      "    accuracy                          0.644       180\n",
      "   macro avg      0.645     0.644     0.644       180\n",
      "weighted avg      0.645     0.644     0.644       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.721 | Valid loss:   0.669 | Valid F1:   0.554\n",
      "Epoch:2 | Train Loss: 0.245 | Valid loss:   0.661 | Valid F1:   0.605\n",
      "Epoch:3 | Train Loss: 0.053 | Valid loss:   0.665 | Valid F1:   0.638\n",
      "Epoch:4 | Train Loss: 0.028 | Valid loss:   0.659 | Valid F1:   0.644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.678     0.678     0.678        90\n",
      "           1      0.678     0.678     0.678        90\n",
      "\n",
      "    accuracy                          0.678       180\n",
      "   macro avg      0.678     0.678     0.678       180\n",
      "weighted avg      0.678     0.678     0.678       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.661 | Valid loss:   0.676 | Valid F1:   0.554\n",
      "Epoch:2 | Train Loss: 0.190 | Valid loss:   0.668 | Valid F1:   0.567\n",
      "Epoch:3 | Train Loss: 0.064 | Valid loss:   0.679 | Valid F1:   0.583\n",
      "Epoch:4 | Train Loss: 0.020 | Valid loss:   0.667 | Valid F1:   0.611\n",
      "Epoch:7 | Train Loss: 0.012 | Valid loss:   0.681 | Valid F1:   0.638\n",
      "Epoch:13 | Train Loss: 0.003 | Valid loss:   0.660 | Valid F1:   0.639\n",
      "Epoch:17 | Train Loss: 0.011 | Valid loss:   0.661 | Valid F1:   0.644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.637     0.644     0.641        90\n",
      "           1      0.640     0.633     0.637        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.763 | Valid loss:   0.677 | Valid F1:   0.560\n",
      "Epoch:2 | Train Loss: 0.207 | Valid loss:   0.676 | Valid F1:   0.582\n",
      "Epoch:5 | Train Loss: 0.026 | Valid loss:   0.676 | Valid F1:   0.594\n",
      "Epoch:8 | Train Loss: 0.005 | Valid loss:   0.681 | Valid F1:   0.600\n",
      "Epoch:11 | Train Loss: 0.001 | Valid loss:   0.669 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.648     0.656     0.652        90\n",
      "           1      0.652     0.644     0.648        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.701 | Valid loss:   0.674 | Valid F1:   0.549\n",
      "Epoch:2 | Train Loss: 0.235 | Valid loss:   0.667 | Valid F1:   0.605\n",
      "Epoch:5 | Train Loss: 0.017 | Valid loss:   0.664 | Valid F1:   0.611\n",
      "Epoch:6 | Train Loss: 0.031 | Valid loss:   0.682 | Valid F1:   0.616\n",
      "Epoch:7 | Train Loss: 0.050 | Valid loss:   0.657 | Valid F1:   0.639\n",
      "Epoch:10 | Train Loss: 0.004 | Valid loss:   0.656 | Valid F1:   0.644\n",
      "Epoch:20 | Train Loss: 0.000 | Valid loss:   0.657 | Valid F1:   0.644\n",
      "Epoch:21 | Train Loss: 0.000 | Valid loss:   0.657 | Valid F1:   0.650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.678     0.670        90\n",
      "           1      0.670     0.656     0.663        90\n",
      "\n",
      "    accuracy                          0.667       180\n",
      "   macro avg      0.667     0.667     0.667       180\n",
      "weighted avg      0.667     0.667     0.667       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.595 | Valid loss:   0.679 | Valid F1:   0.543\n",
      "Epoch:2 | Train Loss: 0.126 | Valid loss:   0.679 | Valid F1:   0.586\n",
      "Epoch:3 | Train Loss: 0.033 | Valid loss:   0.699 | Valid F1:   0.588\n",
      "Epoch:7 | Train Loss: 0.051 | Valid loss:   0.690 | Valid F1:   0.600\n",
      "Epoch:11 | Train Loss: 0.002 | Valid loss:   0.692 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.645     0.667     0.656        90\n",
      "           1      0.655     0.633     0.644        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.620 | Valid loss:   0.675 | Valid F1:   0.555\n",
      "Epoch:2 | Train Loss: 0.152 | Valid loss:   0.676 | Valid F1:   0.593\n",
      "Epoch:5 | Train Loss: 0.018 | Valid loss:   0.682 | Valid F1:   0.594\n",
      "Epoch:6 | Train Loss: 0.009 | Valid loss:   0.674 | Valid F1:   0.606\n",
      "Epoch:7 | Train Loss: 0.003 | Valid loss:   0.679 | Valid F1:   0.616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.667     0.659        90\n",
      "           1      0.659     0.644     0.652        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.607 | Valid loss:   0.684 | Valid F1:   0.550\n",
      "Epoch:3 | Train Loss: 0.198 | Valid loss:   0.710 | Valid F1:   0.578\n",
      "Epoch:5 | Train Loss: 0.053 | Valid loss:   0.711 | Valid F1:   0.599\n",
      "Epoch:9 | Train Loss: 0.018 | Valid loss:   0.687 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.659     0.667     0.663        90\n",
      "           1      0.663     0.656     0.659        90\n",
      "\n",
      "    accuracy                          0.661       180\n",
      "   macro avg      0.661     0.661     0.661       180\n",
      "weighted avg      0.661     0.661     0.661       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.548 | Valid loss:   0.675 | Valid F1:   0.572\n",
      "Epoch:4 | Train Loss: 0.098 | Valid loss:   0.698 | Valid F1:   0.578\n",
      "Epoch:7 | Train Loss: 0.015 | Valid loss:   0.675 | Valid F1:   0.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.678     0.670        90\n",
      "           1      0.670     0.656     0.663        90\n",
      "\n",
      "    accuracy                          0.667       180\n",
      "   macro avg      0.667     0.667     0.667       180\n",
      "weighted avg      0.667     0.667     0.667       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.603 | Valid loss:   0.701 | Valid F1:   0.533\n",
      "Epoch:2 | Train Loss: 0.237 | Valid loss:   0.708 | Valid F1:   0.578\n",
      "Epoch:6 | Train Loss: 0.024 | Valid loss:   0.705 | Valid F1:   0.606\n",
      "Epoch:7 | Train Loss: 0.026 | Valid loss:   0.687 | Valid F1:   0.611\n",
      "Epoch:10 | Train Loss: 0.008 | Valid loss:   0.664 | Valid F1:   0.628\n",
      "Epoch:11 | Train Loss: 0.005 | Valid loss:   0.668 | Valid F1:   0.633\n",
      "Epoch:12 | Train Loss: 0.003 | Valid loss:   0.659 | Valid F1:   0.633\n",
      "Epoch:20 | Train Loss: 0.001 | Valid loss:   0.666 | Valid F1:   0.639\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.656     0.656        90\n",
      "           1      0.656     0.656     0.656        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.627 | Valid loss:   0.701 | Valid F1:   0.566\n",
      "Epoch:3 | Train Loss: 0.098 | Valid loss:   0.716 | Valid F1:   0.583\n",
      "Epoch:5 | Train Loss: 0.085 | Valid loss:   0.687 | Valid F1:   0.605\n",
      "Epoch:7 | Train Loss: 0.022 | Valid loss:   0.677 | Valid F1:   0.622\n",
      "Epoch:10 | Train Loss: 0.007 | Valid loss:   0.675 | Valid F1:   0.622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.644     0.648        90\n",
      "           1      0.648     0.656     0.652        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.759 | Valid loss:   0.685 | Valid F1:   0.517\n",
      "Epoch:2 | Train Loss: 0.228 | Valid loss:   0.685 | Valid F1:   0.527\n",
      "Epoch:4 | Train Loss: 0.037 | Valid loss:   0.687 | Valid F1:   0.544\n",
      "Epoch:7 | Train Loss: 0.016 | Valid loss:   0.694 | Valid F1:   0.544\n",
      "Epoch:8 | Train Loss: 0.096 | Valid loss:   0.682 | Valid F1:   0.551\n",
      "Epoch:13 | Train Loss: 0.005 | Valid loss:   0.711 | Valid F1:   0.561\n",
      "Epoch:14 | Train Loss: 0.004 | Valid loss:   0.708 | Valid F1:   0.567\n",
      "Epoch:23 | Train Loss: 0.017 | Valid loss:   0.702 | Valid F1:   0.583\n",
      "Epoch:24 | Train Loss: 0.008 | Valid loss:   0.704 | Valid F1:   0.589\n",
      "Epoch:27 | Train Loss: 0.002 | Valid loss:   0.710 | Valid F1:   0.600\n",
      "Epoch:499 | Train Loss: 0.002 | Valid loss:   0.722 | Valid F1:   0.604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.593     0.567     0.580        90\n",
      "           1      0.585     0.611     0.598        90\n",
      "\n",
      "    accuracy                          0.589       180\n",
      "   macro avg      0.589     0.589     0.589       180\n",
      "weighted avg      0.589     0.589     0.589       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.684 | Valid loss:   0.669 | Valid F1:   0.546\n",
      "Epoch:2 | Train Loss: 0.174 | Valid loss:   0.669 | Valid F1:   0.588\n",
      "Epoch:8 | Train Loss: 0.311 | Valid loss:   0.674 | Valid F1:   0.621\n",
      "Epoch:9 | Train Loss: 0.058 | Valid loss:   0.664 | Valid F1:   0.627\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.678     0.678     0.678        90\n",
      "           1      0.678     0.678     0.678        90\n",
      "\n",
      "    accuracy                          0.678       180\n",
      "   macro avg      0.678     0.678     0.678       180\n",
      "weighted avg      0.678     0.678     0.678       180\n",
      "\n",
      "New best params: (0.2, 0.0001, 16, 2048, 256) with loss: 0.6154830455780029 and F1 score of :0.6777777777777778\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.810 | Valid loss:   0.675 | Valid F1:   0.511\n",
      "Epoch:2 | Train Loss: 0.219 | Valid loss:   0.671 | Valid F1:   0.532\n",
      "Epoch:3 | Train Loss: 0.083 | Valid loss:   0.673 | Valid F1:   0.554\n",
      "Epoch:7 | Train Loss: 0.014 | Valid loss:   0.660 | Valid F1:   0.611\n",
      "Epoch:25 | Train Loss: 0.001 | Valid loss:   0.684 | Valid F1:   0.617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.615     0.622     0.619        90\n",
      "           1      0.618     0.611     0.615        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.831 | Valid loss:   0.672 | Valid F1:   0.583\n",
      "Epoch:4 | Train Loss: 0.042 | Valid loss:   0.672 | Valid F1:   0.588\n",
      "Epoch:5 | Train Loss: 0.026 | Valid loss:   0.673 | Valid F1:   0.600\n",
      "Epoch:6 | Train Loss: 0.019 | Valid loss:   0.673 | Valid F1:   0.605\n",
      "Epoch:498 | Train Loss: 0.007 | Valid loss:   0.699 | Valid F1:   0.606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.617     0.644     0.630        90\n",
      "           1      0.628     0.600     0.614        90\n",
      "\n",
      "    accuracy                          0.622       180\n",
      "   macro avg      0.622     0.622     0.622       180\n",
      "weighted avg      0.622     0.622     0.622       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.838 | Valid loss:   0.681 | Valid F1:   0.533\n",
      "Epoch:2 | Train Loss: 0.216 | Valid loss:   0.684 | Valid F1:   0.555\n",
      "Epoch:7 | Train Loss: 0.020 | Valid loss:   0.677 | Valid F1:   0.561\n",
      "Epoch:8 | Train Loss: 0.017 | Valid loss:   0.677 | Valid F1:   0.572\n",
      "Epoch:13 | Train Loss: 0.007 | Valid loss:   0.678 | Valid F1:   0.575\n",
      "Epoch:14 | Train Loss: 0.006 | Valid loss:   0.680 | Valid F1:   0.583\n",
      "Epoch:17 | Train Loss: 0.004 | Valid loss:   0.679 | Valid F1:   0.600\n",
      "Epoch:22 | Train Loss: 0.006 | Valid loss:   0.672 | Valid F1:   0.616\n",
      "Epoch:23 | Train Loss: 0.005 | Valid loss:   0.673 | Valid F1:   0.616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.674     0.667     0.670        90\n",
      "           1      0.670     0.678     0.674        90\n",
      "\n",
      "    accuracy                          0.672       180\n",
      "   macro avg      0.672     0.672     0.672       180\n",
      "weighted avg      0.672     0.672     0.672       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.747 | Valid loss:   0.679 | Valid F1:   0.559\n",
      "Epoch:2 | Train Loss: 0.150 | Valid loss:   0.675 | Valid F1:   0.588\n",
      "Epoch:5 | Train Loss: 0.027 | Valid loss:   0.669 | Valid F1:   0.594\n",
      "Epoch:6 | Train Loss: 0.020 | Valid loss:   0.666 | Valid F1:   0.600\n",
      "Epoch:18 | Train Loss: 0.003 | Valid loss:   0.672 | Valid F1:   0.605\n",
      "Epoch:19 | Train Loss: 0.003 | Valid loss:   0.659 | Valid F1:   0.611\n",
      "Epoch:24 | Train Loss: 0.005 | Valid loss:   0.661 | Valid F1:   0.628\n",
      "Epoch:25 | Train Loss: 0.004 | Valid loss:   0.660 | Valid F1:   0.633\n",
      "Epoch:27 | Train Loss: 0.003 | Valid loss:   0.660 | Valid F1:   0.633\n",
      "Epoch:34 | Train Loss: 0.001 | Valid loss:   0.661 | Valid F1:   0.639\n",
      "Epoch:499 | Train Loss: 0.000 | Valid loss:   0.664 | Valid F1:   0.644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.678     0.670        90\n",
      "           1      0.670     0.656     0.663        90\n",
      "\n",
      "    accuracy                          0.667       180\n",
      "   macro avg      0.667     0.667     0.667       180\n",
      "weighted avg      0.667     0.667     0.667       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.833 | Valid loss:   0.676 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.165 | Valid loss:   0.675 | Valid F1:   0.548\n",
      "Epoch:3 | Train Loss: 0.073 | Valid loss:   0.673 | Valid F1:   0.549\n",
      "Epoch:5 | Train Loss: 0.035 | Valid loss:   0.674 | Valid F1:   0.555\n",
      "Epoch:6 | Train Loss: 0.027 | Valid loss:   0.674 | Valid F1:   0.555\n",
      "Epoch:7 | Train Loss: 0.021 | Valid loss:   0.673 | Valid F1:   0.561\n",
      "Epoch:8 | Train Loss: 0.017 | Valid loss:   0.673 | Valid F1:   0.567\n",
      "Epoch:9 | Train Loss: 0.014 | Valid loss:   0.674 | Valid F1:   0.567\n",
      "Epoch:10 | Train Loss: 0.012 | Valid loss:   0.675 | Valid F1:   0.572\n",
      "Epoch:11 | Train Loss: 0.010 | Valid loss:   0.675 | Valid F1:   0.583\n",
      "Epoch:496 | Train Loss: 0.226 | Valid loss:   0.668 | Valid F1:   0.604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.667     0.622     0.644        90\n",
      "           1      0.646     0.689     0.667        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.655       180\n",
      "weighted avg      0.656     0.656     0.655       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.795 | Valid loss:   0.670 | Valid F1:   0.583\n",
      "Epoch:3 | Train Loss: 0.065 | Valid loss:   0.665 | Valid F1:   0.589\n",
      "Epoch:5 | Train Loss: 0.030 | Valid loss:   0.665 | Valid F1:   0.600\n",
      "Epoch:6 | Train Loss: 0.023 | Valid loss:   0.665 | Valid F1:   0.600\n",
      "Epoch:497 | Train Loss: 0.072 | Valid loss:   0.660 | Valid F1:   0.617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.659     0.644     0.652        90\n",
      "           1      0.652     0.667     0.659        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.849 | Valid loss:   0.679 | Valid F1:   0.539\n",
      "Epoch:2 | Train Loss: 0.191 | Valid loss:   0.670 | Valid F1:   0.555\n",
      "Epoch:3 | Train Loss: 0.080 | Valid loss:   0.670 | Valid F1:   0.583\n",
      "Epoch:8 | Train Loss: 0.024 | Valid loss:   0.673 | Valid F1:   0.589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.644     0.644     0.644        90\n",
      "           1      0.644     0.644     0.644        90\n",
      "\n",
      "    accuracy                          0.644       180\n",
      "   macro avg      0.644     0.644     0.644       180\n",
      "weighted avg      0.644     0.644     0.644       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.757 | Valid loss:   0.665 | Valid F1:   0.569\n",
      "Epoch:4 | Train Loss: 0.045 | Valid loss:   0.661 | Valid F1:   0.588\n",
      "Epoch:5 | Train Loss: 0.035 | Valid loss:   0.661 | Valid F1:   0.588\n",
      "Epoch:6 | Train Loss: 0.028 | Valid loss:   0.661 | Valid F1:   0.594\n",
      "Epoch:7 | Train Loss: 0.024 | Valid loss:   0.662 | Valid F1:   0.604\n",
      "Epoch:10 | Train Loss: 0.016 | Valid loss:   0.661 | Valid F1:   0.615\n",
      "Epoch:11 | Train Loss: 0.014 | Valid loss:   0.661 | Valid F1:   0.621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.635     0.600     0.617        90\n",
      "           1      0.621     0.656     0.638        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.627       180\n",
      "weighted avg      0.628     0.628     0.627       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.703 | Valid loss:   0.686 | Valid F1:   0.522\n",
      "Epoch:2 | Train Loss: 0.141 | Valid loss:   0.691 | Valid F1:   0.539\n",
      "Epoch:14 | Train Loss: 0.009 | Valid loss:   0.702 | Valid F1:   0.544\n",
      "Epoch:15 | Train Loss: 0.008 | Valid loss:   0.702 | Valid F1:   0.544\n",
      "Epoch:16 | Train Loss: 0.008 | Valid loss:   0.702 | Valid F1:   0.556\n",
      "Epoch:18 | Train Loss: 0.006 | Valid loss:   0.702 | Valid F1:   0.561\n",
      "Epoch:19 | Train Loss: 0.006 | Valid loss:   0.702 | Valid F1:   0.566\n",
      "Epoch:24 | Train Loss: 0.004 | Valid loss:   0.703 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.670     0.700     0.685        90\n",
      "           1      0.686     0.656     0.670        90\n",
      "\n",
      "    accuracy                          0.678       180\n",
      "   macro avg      0.678     0.678     0.678       180\n",
      "weighted avg      0.678     0.678     0.678       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.625 | Valid loss:   0.667 | Valid F1:   0.555\n",
      "Epoch:3 | Train Loss: 0.058 | Valid loss:   0.671 | Valid F1:   0.559\n",
      "Epoch:4 | Train Loss: 0.040 | Valid loss:   0.672 | Valid F1:   0.559\n",
      "Epoch:5 | Train Loss: 0.031 | Valid loss:   0.673 | Valid F1:   0.560\n",
      "Epoch:12 | Train Loss: 0.010 | Valid loss:   0.681 | Valid F1:   0.560\n",
      "Epoch:15 | Train Loss: 0.007 | Valid loss:   0.682 | Valid F1:   0.566\n",
      "Epoch:37 | Train Loss: 0.001 | Valid loss:   0.692 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.640     0.611     0.625        90\n",
      "           1      0.628     0.656     0.641        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.634     0.633     0.633       180\n",
      "weighted avg      0.634     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.610 | Valid loss:   0.681 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.182 | Valid loss:   0.681 | Valid F1:   0.544\n",
      "Epoch:3 | Train Loss: 0.097 | Valid loss:   0.678 | Valid F1:   0.550\n",
      "Epoch:5 | Train Loss: 0.051 | Valid loss:   0.673 | Valid F1:   0.577\n",
      "Epoch:6 | Train Loss: 0.042 | Valid loss:   0.675 | Valid F1:   0.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.629     0.678     0.652        90\n",
      "           1      0.651     0.600     0.624        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.640     0.639     0.638       180\n",
      "weighted avg      0.640     0.639     0.638       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.581 | Valid loss:   0.673 | Valid F1:   0.553\n",
      "Epoch:2 | Train Loss: 0.171 | Valid loss:   0.671 | Valid F1:   0.565\n",
      "Epoch:3 | Train Loss: 0.088 | Valid loss:   0.667 | Valid F1:   0.573\n",
      "Epoch:4 | Train Loss: 0.059 | Valid loss:   0.667 | Valid F1:   0.592\n",
      "Epoch:496 | Train Loss: 0.004 | Valid loss:   0.681 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.630     0.567     0.596        90\n",
      "           1      0.606     0.667     0.635        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.618     0.617     0.616       180\n",
      "weighted avg      0.618     0.617     0.616       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.585 | Valid loss:   0.674 | Valid F1:   0.609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.655     0.633     0.644        90\n",
      "           1      0.645     0.667     0.656        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.2, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.620 | Valid loss:   0.680 | Valid F1:   0.571\n",
      "Epoch:3 | Train Loss: 0.128 | Valid loss:   0.675 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.609     0.589     0.599        90\n",
      "           1      0.602     0.622     0.612        90\n",
      "\n",
      "    accuracy                          0.606       180\n",
      "   macro avg      0.606     0.606     0.605       180\n",
      "weighted avg      0.606     0.606     0.605       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.650 | Valid loss:   0.673 | Valid F1:   0.563\n",
      "Epoch:2 | Train Loss: 0.218 | Valid loss:   0.693 | Valid F1:   0.572\n",
      "Epoch:3 | Train Loss: 0.040 | Valid loss:   0.683 | Valid F1:   0.593\n",
      "Epoch:13 | Train Loss: 0.004 | Valid loss:   0.702 | Valid F1:   0.605\n",
      "Epoch:14 | Train Loss: 0.001 | Valid loss:   0.719 | Valid F1:   0.605\n",
      "Epoch:17 | Train Loss: 0.009 | Valid loss:   0.709 | Valid F1:   0.627\n",
      "Epoch:18 | Train Loss: 0.001 | Valid loss:   0.688 | Valid F1:   0.628\n",
      "Epoch:19 | Train Loss: 0.000 | Valid loss:   0.705 | Valid F1:   0.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.594     0.633     0.613        90\n",
      "           1      0.607     0.567     0.586        90\n",
      "\n",
      "    accuracy                          0.600       180\n",
      "   macro avg      0.600     0.600     0.600       180\n",
      "weighted avg      0.600     0.600     0.600       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.758 | Valid loss:   0.678 | Valid F1:   0.583\n",
      "Epoch:2 | Train Loss: 0.195 | Valid loss:   0.673 | Valid F1:   0.594\n",
      "Epoch:4 | Train Loss: 0.016 | Valid loss:   0.674 | Valid F1:   0.600\n",
      "Epoch:5 | Train Loss: 0.057 | Valid loss:   0.677 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.648     0.656     0.652        90\n",
      "           1      0.652     0.644     0.648        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.785 | Valid loss:   0.664 | Valid F1:   0.577\n",
      "Epoch:3 | Train Loss: 0.055 | Valid loss:   0.713 | Valid F1:   0.578\n",
      "Epoch:4 | Train Loss: 0.108 | Valid loss:   0.685 | Valid F1:   0.594\n",
      "Epoch:7 | Train Loss: 0.006 | Valid loss:   0.687 | Valid F1:   0.605\n",
      "Epoch:498 | Train Loss: 0.000 | Valid loss:   0.714 | Valid F1:   0.606\n",
      "Epoch:500 | Train Loss: 0.000 | Valid loss:   0.712 | Valid F1:   0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.645     0.667     0.656        90\n",
      "           1      0.655     0.633     0.644        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.584 | Valid loss:   0.677 | Valid F1:   0.548\n",
      "Epoch:2 | Train Loss: 0.157 | Valid loss:   0.719 | Valid F1:   0.561\n",
      "Epoch:5 | Train Loss: 0.016 | Valid loss:   0.722 | Valid F1:   0.561\n",
      "Epoch:6 | Train Loss: 0.038 | Valid loss:   0.713 | Valid F1:   0.564\n",
      "Epoch:13 | Train Loss: 0.012 | Valid loss:   0.707 | Valid F1:   0.583\n",
      "Epoch:16 | Train Loss: 0.019 | Valid loss:   0.728 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.621     0.656     0.638        90\n",
      "           1      0.635     0.600     0.617        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.627       180\n",
      "weighted avg      0.628     0.628     0.627       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.664 | Valid loss:   0.663 | Valid F1:   0.556\n",
      "Epoch:2 | Train Loss: 0.152 | Valid loss:   0.661 | Valid F1:   0.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.663     0.633     0.648        90\n",
      "           1      0.649     0.678     0.663        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.655       180\n",
      "weighted avg      0.656     0.656     0.655       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.686 | Valid loss:   0.658 | Valid F1:   0.576\n",
      "Epoch:2 | Train Loss: 0.174 | Valid loss:   0.673 | Valid F1:   0.583\n",
      "Epoch:3 | Train Loss: 0.032 | Valid loss:   0.683 | Valid F1:   0.583\n",
      "Epoch:4 | Train Loss: 0.009 | Valid loss:   0.685 | Valid F1:   0.611\n",
      "Epoch:8 | Train Loss: 0.003 | Valid loss:   0.664 | Valid F1:   0.627\n",
      "Epoch:10 | Train Loss: 0.001 | Valid loss:   0.658 | Valid F1:   0.644\n",
      "Epoch:11 | Train Loss: 0.001 | Valid loss:   0.665 | Valid F1:   0.650\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.622     0.622     0.622        90\n",
      "           1      0.622     0.622     0.622        90\n",
      "\n",
      "    accuracy                          0.622       180\n",
      "   macro avg      0.622     0.622     0.622       180\n",
      "weighted avg      0.622     0.622     0.622       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.674 | Valid loss:   0.684 | Valid F1:   0.522\n",
      "Epoch:2 | Train Loss: 0.089 | Valid loss:   0.683 | Valid F1:   0.594\n",
      "Epoch:9 | Train Loss: 0.006 | Valid loss:   0.678 | Valid F1:   0.606\n",
      "Epoch:10 | Train Loss: 0.002 | Valid loss:   0.674 | Valid F1:   0.622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.678     0.678     0.678        90\n",
      "           1      0.678     0.678     0.678        90\n",
      "\n",
      "    accuracy                          0.678       180\n",
      "   macro avg      0.678     0.678     0.678       180\n",
      "weighted avg      0.678     0.678     0.678       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.712 | Valid loss:   0.669 | Valid F1:   0.579\n",
      "Epoch:2 | Train Loss: 0.095 | Valid loss:   0.678 | Valid F1:   0.600\n",
      "Epoch:4 | Train Loss: 0.007 | Valid loss:   0.690 | Valid F1:   0.605\n",
      "Epoch:6 | Train Loss: 0.010 | Valid loss:   0.672 | Valid F1:   0.611\n",
      "Epoch:10 | Train Loss: 0.004 | Valid loss:   0.665 | Valid F1:   0.622\n",
      "Epoch:500 | Train Loss: 0.000 | Valid loss:   0.677 | Valid F1:   0.628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.633     0.633     0.633        90\n",
      "           1      0.633     0.633     0.633        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.776 | Valid loss:   0.681 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.115 | Valid loss:   0.698 | Valid F1:   0.550\n",
      "Epoch:5 | Train Loss: 0.006 | Valid loss:   0.710 | Valid F1:   0.578\n",
      "Epoch:18 | Train Loss: 0.000 | Valid loss:   0.709 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.628     0.600     0.614        90\n",
      "           1      0.617     0.644     0.630        90\n",
      "\n",
      "    accuracy                          0.622       180\n",
      "   macro avg      0.622     0.622     0.622       180\n",
      "weighted avg      0.622     0.622     0.622       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.789 | Valid loss:   0.687 | Valid F1:   0.539\n",
      "Epoch:2 | Train Loss: 0.139 | Valid loss:   0.696 | Valid F1:   0.544\n",
      "Epoch:3 | Train Loss: 0.027 | Valid loss:   0.694 | Valid F1:   0.578\n",
      "Epoch:4 | Train Loss: 0.013 | Valid loss:   0.660 | Valid F1:   0.611\n",
      "Epoch:10 | Train Loss: 0.001 | Valid loss:   0.676 | Valid F1:   0.617\n",
      "Epoch:11 | Train Loss: 0.001 | Valid loss:   0.672 | Valid F1:   0.628\n",
      "Epoch:18 | Train Loss: 0.000 | Valid loss:   0.670 | Valid F1:   0.628\n",
      "Epoch:499 | Train Loss: 0.000 | Valid loss:   0.669 | Valid F1:   0.633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.651     0.622     0.636        90\n",
      "           1      0.638     0.667     0.652        90\n",
      "\n",
      "    accuracy                          0.644       180\n",
      "   macro avg      0.645     0.644     0.644       180\n",
      "weighted avg      0.645     0.644     0.644       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.636 | Valid loss:   0.691 | Valid F1:   0.528\n",
      "Epoch:2 | Train Loss: 0.073 | Valid loss:   0.691 | Valid F1:   0.556\n",
      "Epoch:3 | Train Loss: 0.013 | Valid loss:   0.701 | Valid F1:   0.567\n",
      "Epoch:8 | Train Loss: 0.004 | Valid loss:   0.674 | Valid F1:   0.611\n",
      "Epoch:9 | Train Loss: 0.009 | Valid loss:   0.669 | Valid F1:   0.638\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.667     0.622     0.644        90\n",
      "           1      0.646     0.689     0.667        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.655       180\n",
      "weighted avg      0.656     0.656     0.655       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.635 | Valid loss:   0.677 | Valid F1:   0.548\n",
      "Epoch:3 | Train Loss: 0.018 | Valid loss:   0.707 | Valid F1:   0.560\n",
      "Epoch:5 | Train Loss: 0.005 | Valid loss:   0.687 | Valid F1:   0.578\n",
      "Epoch:8 | Train Loss: 0.002 | Valid loss:   0.705 | Valid F1:   0.589\n",
      "Epoch:9 | Train Loss: 0.001 | Valid loss:   0.703 | Valid F1:   0.594\n",
      "Epoch:10 | Train Loss: 0.001 | Valid loss:   0.702 | Valid F1:   0.594\n",
      "Epoch:11 | Train Loss: 0.001 | Valid loss:   0.703 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.709     0.678     0.693        90\n",
      "           1      0.691     0.722     0.707        90\n",
      "\n",
      "    accuracy                          0.700       180\n",
      "   macro avg      0.700     0.700     0.700       180\n",
      "weighted avg      0.700     0.700     0.700       180\n",
      "\n",
      "New best params: (0.3, 0.001, 64, 1024, 256) with loss: 0.6113027930259705 and F1 score of :0.6998517786561265\n",
      "Training with dropout=0.3, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.646 | Valid loss:   0.687 | Valid F1:   0.506\n",
      "Epoch:2 | Train Loss: 0.154 | Valid loss:   0.685 | Valid F1:   0.544\n",
      "Epoch:3 | Train Loss: 0.045 | Valid loss:   0.715 | Valid F1:   0.566\n",
      "Epoch:7 | Train Loss: 0.016 | Valid loss:   0.697 | Valid F1:   0.599\n",
      "Epoch:10 | Train Loss: 0.005 | Valid loss:   0.675 | Valid F1:   0.622\n",
      "Epoch:11 | Train Loss: 0.009 | Valid loss:   0.647 | Valid F1:   0.644\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.648     0.633     0.640        90\n",
      "           1      0.641     0.656     0.648        90\n",
      "\n",
      "    accuracy                          0.644       180\n",
      "   macro avg      0.645     0.644     0.644       180\n",
      "weighted avg      0.645     0.644     0.644       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.604 | Valid loss:   0.692 | Valid F1:   0.578\n",
      "Epoch:10 | Train Loss: 0.011 | Valid loss:   0.677 | Valid F1:   0.583\n",
      "Epoch:11 | Train Loss: 0.008 | Valid loss:   0.703 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.615     0.622     0.619        90\n",
      "           1      0.618     0.611     0.615        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.568 | Valid loss:   0.697 | Valid F1:   0.533\n",
      "Epoch:2 | Train Loss: 0.107 | Valid loss:   0.693 | Valid F1:   0.561\n",
      "Epoch:3 | Train Loss: 0.030 | Valid loss:   0.689 | Valid F1:   0.578\n",
      "Epoch:8 | Train Loss: 0.020 | Valid loss:   0.681 | Valid F1:   0.582\n",
      "Epoch:9 | Train Loss: 0.123 | Valid loss:   0.681 | Valid F1:   0.589\n",
      "Epoch:11 | Train Loss: 0.020 | Valid loss:   0.691 | Valid F1:   0.615\n",
      "Epoch:12 | Train Loss: 0.013 | Valid loss:   0.690 | Valid F1:   0.615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.633     0.633     0.633        90\n",
      "           1      0.633     0.633     0.633        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.647 | Valid loss:   0.703 | Valid F1:   0.561\n",
      "Epoch:2 | Train Loss: 0.101 | Valid loss:   0.684 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.611     0.615        90\n",
      "           1      0.615     0.622     0.619        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.725 | Valid loss:   0.676 | Valid F1:   0.550\n",
      "Epoch:5 | Train Loss: 0.022 | Valid loss:   0.682 | Valid F1:   0.550\n",
      "Epoch:7 | Train Loss: 0.011 | Valid loss:   0.688 | Valid F1:   0.556\n",
      "Epoch:9 | Train Loss: 0.006 | Valid loss:   0.690 | Valid F1:   0.567\n",
      "Epoch:12 | Train Loss: 0.157 | Valid loss:   0.673 | Valid F1:   0.578\n",
      "Epoch:15 | Train Loss: 0.006 | Valid loss:   0.679 | Valid F1:   0.611\n",
      "Epoch:21 | Train Loss: 0.001 | Valid loss:   0.684 | Valid F1:   0.617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.678     0.656     0.667        90\n",
      "           1      0.667     0.689     0.678        90\n",
      "\n",
      "    accuracy                          0.672       180\n",
      "   macro avg      0.672     0.672     0.672       180\n",
      "weighted avg      0.672     0.672     0.672       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.815 | Valid loss:   0.676 | Valid F1:   0.553\n",
      "Epoch:2 | Train Loss: 0.194 | Valid loss:   0.676 | Valid F1:   0.583\n",
      "Epoch:14 | Train Loss: 0.009 | Valid loss:   0.689 | Valid F1:   0.589\n",
      "Epoch:15 | Train Loss: 0.005 | Valid loss:   0.686 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.659     0.644     0.652        90\n",
      "           1      0.652     0.667     0.659        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.757 | Valid loss:   0.676 | Valid F1:   0.571\n",
      "Epoch:2 | Train Loss: 0.140 | Valid loss:   0.676 | Valid F1:   0.578\n",
      "Epoch:3 | Train Loss: 0.064 | Valid loss:   0.683 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.626     0.633     0.630        90\n",
      "           1      0.629     0.622     0.626        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.628       180\n",
      "weighted avg      0.628     0.628     0.628       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.710 | Valid loss:   0.685 | Valid F1:   0.550\n",
      "Epoch:14 | Train Loss: 0.013 | Valid loss:   0.698 | Valid F1:   0.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.640     0.611     0.625        90\n",
      "           1      0.628     0.656     0.641        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.634     0.633     0.633       180\n",
      "weighted avg      0.634     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.774 | Valid loss:   0.669 | Valid F1:   0.532\n",
      "Epoch:2 | Train Loss: 0.142 | Valid loss:   0.673 | Valid F1:   0.543\n",
      "Epoch:3 | Train Loss: 0.062 | Valid loss:   0.676 | Valid F1:   0.570\n",
      "Epoch:6 | Train Loss: 0.024 | Valid loss:   0.678 | Valid F1:   0.571\n",
      "Epoch:11 | Train Loss: 0.009 | Valid loss:   0.681 | Valid F1:   0.571\n",
      "Epoch:12 | Train Loss: 0.008 | Valid loss:   0.681 | Valid F1:   0.576\n",
      "Epoch:24 | Train Loss: 0.147 | Valid loss:   0.673 | Valid F1:   0.605\n",
      "Epoch:497 | Train Loss: 0.000 | Valid loss:   0.671 | Valid F1:   0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.688     0.611     0.647        90\n",
      "           1      0.650     0.722     0.684        90\n",
      "\n",
      "    accuracy                          0.667       180\n",
      "   macro avg      0.669     0.667     0.666       180\n",
      "weighted avg      0.669     0.667     0.666       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.703 | Valid loss:   0.676 | Valid F1:   0.558\n",
      "Epoch:2 | Train Loss: 0.129 | Valid loss:   0.671 | Valid F1:   0.582\n",
      "Epoch:4 | Train Loss: 0.035 | Valid loss:   0.670 | Valid F1:   0.588\n",
      "Epoch:9 | Train Loss: 0.011 | Valid loss:   0.673 | Valid F1:   0.594\n",
      "Epoch:22 | Train Loss: 0.002 | Valid loss:   0.676 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.640     0.633     0.637        90\n",
      "           1      0.637     0.644     0.641        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.997 | Valid loss:   0.683 | Valid F1:   0.550\n",
      "Epoch:4 | Train Loss: 0.063 | Valid loss:   0.688 | Valid F1:   0.555\n",
      "Epoch:10 | Train Loss: 0.014 | Valid loss:   0.691 | Valid F1:   0.556\n",
      "Epoch:12 | Train Loss: 0.010 | Valid loss:   0.692 | Valid F1:   0.567\n",
      "Epoch:498 | Train Loss: 0.048 | Valid loss:   0.699 | Valid F1:   0.572\n",
      "Epoch:500 | Train Loss: 0.007 | Valid loss:   0.697 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.618     0.611     0.615        90\n",
      "           1      0.615     0.622     0.619        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.767 | Valid loss:   0.680 | Valid F1:   0.555\n",
      "Epoch:2 | Train Loss: 0.111 | Valid loss:   0.676 | Valid F1:   0.572\n",
      "Epoch:3 | Train Loss: 0.054 | Valid loss:   0.676 | Valid F1:   0.583\n",
      "Epoch:8 | Train Loss: 0.013 | Valid loss:   0.674 | Valid F1:   0.594\n",
      "Epoch:10 | Train Loss: 0.008 | Valid loss:   0.675 | Valid F1:   0.599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.652     0.667     0.659        90\n",
      "           1      0.659     0.644     0.652        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.825 | Valid loss:   0.681 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.152 | Valid loss:   0.682 | Valid F1:   0.544\n",
      "Epoch:3 | Train Loss: 0.063 | Valid loss:   0.683 | Valid F1:   0.561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.602     0.589     0.596        90\n",
      "           1      0.598     0.611     0.604        90\n",
      "\n",
      "    accuracy                          0.600       180\n",
      "   macro avg      0.600     0.600     0.600       180\n",
      "weighted avg      0.600     0.600     0.600       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.813 | Valid loss:   0.667 | Valid F1:   0.576\n",
      "Epoch:2 | Train Loss: 0.160 | Valid loss:   0.662 | Valid F1:   0.611\n",
      "Epoch:3 | Train Loss: 0.065 | Valid loss:   0.662 | Valid F1:   0.611\n",
      "Epoch:4 | Train Loss: 0.045 | Valid loss:   0.662 | Valid F1:   0.617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.656     0.656        90\n",
      "           1      0.656     0.656     0.656        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.825 | Valid loss:   0.696 | Valid F1:   0.533\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.633     0.633     0.633        90\n",
      "           1      0.633     0.633     0.633        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.618 | Valid loss:   0.674 | Valid F1:   0.556\n",
      "Epoch:4 | Train Loss: 0.035 | Valid loss:   0.679 | Valid F1:   0.560\n",
      "Epoch:497 | Train Loss: 0.001 | Valid loss:   0.697 | Valid F1:   0.566\n",
      "Epoch:499 | Train Loss: 0.001 | Valid loss:   0.697 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.644     0.622     0.633        90\n",
      "           1      0.634     0.656     0.645        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.640 | Valid loss:   0.676 | Valid F1:   0.561\n",
      "Epoch:2 | Train Loss: 0.146 | Valid loss:   0.673 | Valid F1:   0.594\n",
      "Epoch:13 | Train Loss: 0.017 | Valid loss:   0.673 | Valid F1:   0.600\n",
      "Epoch:14 | Train Loss: 0.016 | Valid loss:   0.673 | Valid F1:   0.605\n",
      "Epoch:15 | Train Loss: 0.014 | Valid loss:   0.673 | Valid F1:   0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.600     0.567     0.583        90\n",
      "           1      0.589     0.622     0.605        90\n",
      "\n",
      "    accuracy                          0.594       180\n",
      "   macro avg      0.595     0.594     0.594       180\n",
      "weighted avg      0.595     0.594     0.594       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.588 | Valid loss:   0.674 | Valid F1:   0.584\n",
      "Epoch:13 | Train Loss: 0.014 | Valid loss:   0.684 | Valid F1:   0.586\n",
      "Epoch:16 | Train Loss: 0.011 | Valid loss:   0.685 | Valid F1:   0.587\n",
      "Epoch:34 | Train Loss: 0.004 | Valid loss:   0.691 | Valid F1:   0.588\n",
      "Epoch:36 | Train Loss: 0.003 | Valid loss:   0.692 | Valid F1:   0.593\n",
      "Epoch:43 | Train Loss: 0.003 | Valid loss:   0.694 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.640     0.633     0.637        90\n",
      "           1      0.637     0.644     0.641        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.667 | Valid loss:   0.701 | Valid F1:   0.498\n",
      "Epoch:3 | Train Loss: 0.124 | Valid loss:   0.707 | Valid F1:   0.499\n",
      "Epoch:5 | Train Loss: 0.067 | Valid loss:   0.710 | Valid F1:   0.511\n",
      "Epoch:6 | Train Loss: 0.054 | Valid loss:   0.711 | Valid F1:   0.516\n",
      "Epoch:11 | Train Loss: 0.025 | Valid loss:   0.715 | Valid F1:   0.522\n",
      "Epoch:12 | Train Loss: 0.023 | Valid loss:   0.715 | Valid F1:   0.527\n",
      "Epoch:18 | Train Loss: 0.013 | Valid loss:   0.717 | Valid F1:   0.528\n",
      "Epoch:21 | Train Loss: 0.011 | Valid loss:   0.717 | Valid F1:   0.533\n",
      "Epoch:22 | Train Loss: 0.010 | Valid loss:   0.717 | Valid F1:   0.539\n",
      "Epoch:23 | Train Loss: 0.009 | Valid loss:   0.717 | Valid F1:   0.544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.646     0.689     0.667        90\n",
      "           1      0.667     0.622     0.644        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.655       180\n",
      "weighted avg      0.656     0.656     0.655       180\n",
      "\n",
      "Training with dropout=0.3, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.611 | Valid loss:   0.689 | Valid F1:   0.533\n",
      "Epoch:2 | Train Loss: 0.180 | Valid loss:   0.690 | Valid F1:   0.539\n",
      "Epoch:3 | Train Loss: 0.107 | Valid loss:   0.691 | Valid F1:   0.549\n",
      "Epoch:4 | Train Loss: 0.076 | Valid loss:   0.692 | Valid F1:   0.550\n",
      "Epoch:5 | Train Loss: 0.059 | Valid loss:   0.692 | Valid F1:   0.550\n",
      "Epoch:6 | Train Loss: 0.048 | Valid loss:   0.692 | Valid F1:   0.567\n",
      "Epoch:15 | Train Loss: 0.015 | Valid loss:   0.695 | Valid F1:   0.572\n",
      "Epoch:22 | Train Loss: 0.009 | Valid loss:   0.695 | Valid F1:   0.578\n",
      "Epoch:23 | Train Loss: 0.008 | Valid loss:   0.695 | Valid F1:   0.583\n",
      "Epoch:29 | Train Loss: 0.006 | Valid loss:   0.697 | Valid F1:   0.589\n",
      "Epoch:35 | Train Loss: 0.004 | Valid loss:   0.697 | Valid F1:   0.594\n",
      "Epoch:43 | Train Loss: 0.003 | Valid loss:   0.698 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.621     0.600     0.610        90\n",
      "           1      0.613     0.633     0.623        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.722 | Valid loss:   0.680 | Valid F1:   0.592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.659     0.667     0.663        90\n",
      "           1      0.663     0.656     0.659        90\n",
      "\n",
      "    accuracy                          0.661       180\n",
      "   macro avg      0.661     0.661     0.661       180\n",
      "weighted avg      0.661     0.661     0.661       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.851 | Valid loss:   0.677 | Valid F1:   0.548\n",
      "Epoch:2 | Train Loss: 0.113 | Valid loss:   0.706 | Valid F1:   0.566\n",
      "Epoch:5 | Train Loss: 0.005 | Valid loss:   0.688 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.630     0.644     0.637        90\n",
      "           1      0.636     0.622     0.629        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.738 | Valid loss:   0.679 | Valid F1:   0.527\n",
      "Epoch:2 | Train Loss: 0.110 | Valid loss:   0.713 | Valid F1:   0.539\n",
      "Epoch:3 | Train Loss: 0.015 | Valid loss:   0.713 | Valid F1:   0.544\n",
      "Epoch:5 | Train Loss: 0.003 | Valid loss:   0.716 | Valid F1:   0.550\n",
      "Epoch:8 | Train Loss: 0.001 | Valid loss:   0.744 | Valid F1:   0.556\n",
      "Epoch:10 | Train Loss: 0.000 | Valid loss:   0.747 | Valid F1:   0.561\n",
      "Epoch:12 | Train Loss: 0.000 | Valid loss:   0.750 | Valid F1:   0.566\n",
      "Epoch:496 | Train Loss: 0.000 | Valid loss:   0.752 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.607     0.600     0.603        90\n",
      "           1      0.604     0.611     0.608        90\n",
      "\n",
      "    accuracy                          0.606       180\n",
      "   macro avg      0.606     0.606     0.606       180\n",
      "weighted avg      0.606     0.606     0.606       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.758 | Valid loss:   0.681 | Valid F1:   0.564\n",
      "Epoch:5 | Train Loss: 0.003 | Valid loss:   0.703 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.671     0.589     0.627        90\n",
      "           1      0.634     0.711     0.670        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.652     0.650     0.649       180\n",
      "weighted avg      0.652     0.650     0.649       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.786 | Valid loss:   0.665 | Valid F1:   0.561\n",
      "Epoch:4 | Train Loss: 0.006 | Valid loss:   0.681 | Valid F1:   0.594\n",
      "Epoch:6 | Train Loss: 0.002 | Valid loss:   0.670 | Valid F1:   0.621\n",
      "Epoch:7 | Train Loss: 0.002 | Valid loss:   0.673 | Valid F1:   0.622\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.690     0.667     0.678        90\n",
      "           1      0.677     0.700     0.689        90\n",
      "\n",
      "    accuracy                          0.683       180\n",
      "   macro avg      0.684     0.683     0.683       180\n",
      "weighted avg      0.684     0.683     0.683       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.846 | Valid loss:   0.669 | Valid F1:   0.567\n",
      "Epoch:2 | Train Loss: 0.099 | Valid loss:   0.676 | Valid F1:   0.583\n",
      "Epoch:4 | Train Loss: 0.006 | Valid loss:   0.683 | Valid F1:   0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.635     0.600     0.617        90\n",
      "           1      0.621     0.656     0.638        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.627       180\n",
      "weighted avg      0.628     0.628     0.627       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.733 | Valid loss:   0.676 | Valid F1:   0.561\n",
      "Epoch:3 | Train Loss: 0.014 | Valid loss:   0.682 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.611     0.621        90\n",
      "           1      0.624     0.644     0.634        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.628       180\n",
      "weighted avg      0.628     0.628     0.628       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.764 | Valid loss:   0.685 | Valid F1:   0.537\n",
      "Epoch:4 | Train Loss: 0.006 | Valid loss:   0.701 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.626     0.633     0.630        90\n",
      "           1      0.629     0.622     0.626        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.628       180\n",
      "weighted avg      0.628     0.628     0.628       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.752 | Valid loss:   0.675 | Valid F1:   0.589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.615     0.622     0.619        90\n",
      "           1      0.618     0.611     0.615        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.942 | Valid loss:   0.689 | Valid F1:   0.570\n",
      "Epoch:3 | Train Loss: 0.020 | Valid loss:   0.678 | Valid F1:   0.583\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.600     0.600     0.600        90\n",
      "           1      0.600     0.600     0.600        90\n",
      "\n",
      "    accuracy                          0.600       180\n",
      "   macro avg      0.600     0.600     0.600       180\n",
      "weighted avg      0.600     0.600     0.600       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.695 | Valid loss:   0.695 | Valid F1:   0.528\n",
      "Epoch:2 | Train Loss: 0.077 | Valid loss:   0.696 | Valid F1:   0.550\n",
      "Epoch:3 | Train Loss: 0.015 | Valid loss:   0.701 | Valid F1:   0.561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.619     0.578     0.598        90\n",
      "           1      0.604     0.644     0.624        90\n",
      "\n",
      "    accuracy                          0.611       180\n",
      "   macro avg      0.612     0.611     0.611       180\n",
      "weighted avg      0.612     0.611     0.611       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.811 | Valid loss:   0.682 | Valid F1:   0.589\n",
      "Epoch:2 | Train Loss: 0.067 | Valid loss:   0.682 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.631     0.589     0.609        90\n",
      "           1      0.615     0.656     0.634        90\n",
      "\n",
      "    accuracy                          0.622       180\n",
      "   macro avg      0.623     0.622     0.622       180\n",
      "weighted avg      0.623     0.622     0.622       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.632 | Valid loss:   0.705 | Valid F1:   0.522\n",
      "Epoch:3 | Train Loss: 0.027 | Valid loss:   0.705 | Valid F1:   0.539\n",
      "Epoch:7 | Train Loss: 0.005 | Valid loss:   0.724 | Valid F1:   0.544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.620     0.633     0.626        90\n",
      "           1      0.625     0.611     0.618        90\n",
      "\n",
      "    accuracy                          0.622       180\n",
      "   macro avg      0.622     0.622     0.622       180\n",
      "weighted avg      0.622     0.622     0.622       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.585 | Valid loss:   0.693 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.073 | Valid loss:   0.682 | Valid F1:   0.600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.671     0.633     0.651        90\n",
      "           1      0.653     0.689     0.670        90\n",
      "\n",
      "    accuracy                          0.661       180\n",
      "   macro avg      0.662     0.661     0.661       180\n",
      "weighted avg      0.662     0.661     0.661       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.606 | Valid loss:   0.688 | Valid F1:   0.561\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.655     0.633     0.644        90\n",
      "           1      0.645     0.667     0.656        90\n",
      "\n",
      "    accuracy                          0.650       180\n",
      "   macro avg      0.650     0.650     0.650       180\n",
      "weighted avg      0.650     0.650     0.650       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.720 | Valid loss:   0.684 | Valid F1:   0.560\n",
      "Epoch:2 | Train Loss: 0.066 | Valid loss:   0.682 | Valid F1:   0.578\n",
      "Epoch:5 | Train Loss: 0.009 | Valid loss:   0.689 | Valid F1:   0.594\n",
      "Epoch:6 | Train Loss: 0.007 | Valid loss:   0.690 | Valid F1:   0.600\n",
      "Epoch:7 | Train Loss: 0.005 | Valid loss:   0.692 | Valid F1:   0.606\n",
      "Epoch:9 | Train Loss: 0.003 | Valid loss:   0.694 | Valid F1:   0.611\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.636     0.622     0.629        90\n",
      "           1      0.630     0.644     0.637        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.850 | Valid loss:   0.700 | Valid F1:   0.498\n",
      "Epoch:3 | Train Loss: 0.059 | Valid loss:   0.704 | Valid F1:   0.516\n",
      "Epoch:5 | Train Loss: 0.025 | Valid loss:   0.707 | Valid F1:   0.517\n",
      "Epoch:7 | Train Loss: 0.012 | Valid loss:   0.706 | Valid F1:   0.522\n",
      "Epoch:9 | Train Loss: 0.007 | Valid loss:   0.709 | Valid F1:   0.528\n",
      "Epoch:10 | Train Loss: 0.005 | Valid loss:   0.707 | Valid F1:   0.533\n",
      "Epoch:11 | Train Loss: 0.004 | Valid loss:   0.711 | Valid F1:   0.544\n",
      "Epoch:15 | Train Loss: 0.008 | Valid loss:   0.706 | Valid F1:   0.548\n",
      "Epoch:18 | Train Loss: 0.001 | Valid loss:   0.701 | Valid F1:   0.550\n",
      "Epoch:496 | Train Loss: 0.000 | Valid loss:   0.712 | Valid F1:   0.556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.633     0.633     0.633        90\n",
      "           1      0.633     0.633     0.633        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=16, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 1.051 | Valid loss:   0.689 | Valid F1:   0.542\n",
      "Epoch:16 | Train Loss: 0.059 | Valid loss:   0.686 | Valid F1:   0.566\n",
      "Epoch:17 | Train Loss: 0.032 | Valid loss:   0.693 | Valid F1:   0.594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.656     0.656     0.656        90\n",
      "           1      0.656     0.656     0.656        90\n",
      "\n",
      "    accuracy                          0.656       180\n",
      "   macro avg      0.656     0.656     0.656       180\n",
      "weighted avg      0.656     0.656     0.656       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.866 | Valid loss:   0.675 | Valid F1:   0.570\n",
      "Epoch:2 | Train Loss: 0.184 | Valid loss:   0.670 | Valid F1:   0.582\n",
      "Epoch:3 | Train Loss: 0.086 | Valid loss:   0.674 | Valid F1:   0.593\n",
      "Epoch:4 | Train Loss: 0.053 | Valid loss:   0.676 | Valid F1:   0.593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.621     0.600     0.610        90\n",
      "           1      0.613     0.633     0.623        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.617       180\n",
      "weighted avg      0.617     0.617     0.617       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=16, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.799 | Valid loss:   0.689 | Valid F1:   0.544\n",
      "Epoch:498 | Train Loss: 0.001 | Valid loss:   0.735 | Valid F1:   0.544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.624     0.589     0.606        90\n",
      "           1      0.611     0.644     0.627        90\n",
      "\n",
      "    accuracy                          0.617       180\n",
      "   macro avg      0.617     0.617     0.616       180\n",
      "weighted avg      0.617     0.617     0.616       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.867 | Valid loss:   0.685 | Valid F1:   0.544\n",
      "Epoch:2 | Train Loss: 0.136 | Valid loss:   0.687 | Valid F1:   0.548\n",
      "Epoch:3 | Train Loss: 0.065 | Valid loss:   0.689 | Valid F1:   0.572\n",
      "Epoch:19 | Train Loss: 0.003 | Valid loss:   0.698 | Valid F1:   0.578\n",
      "Epoch:499 | Train Loss: 0.003 | Valid loss:   0.679 | Valid F1:   0.583\n",
      "Epoch:500 | Train Loss: 0.002 | Valid loss:   0.680 | Valid F1:   0.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.632     0.611     0.621        90\n",
      "           1      0.624     0.644     0.634        90\n",
      "\n",
      "    accuracy                          0.628       180\n",
      "   macro avg      0.628     0.628     0.628       180\n",
      "weighted avg      0.628     0.628     0.628       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=32, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.948 | Valid loss:   0.675 | Valid F1:   0.559\n",
      "Epoch:23 | Train Loss: 0.001 | Valid loss:   0.694 | Valid F1:   0.561\n",
      "Epoch:499 | Train Loss: 0.001 | Valid loss:   0.684 | Valid F1:   0.589\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.604     0.611     0.608        90\n",
      "           1      0.607     0.600     0.603        90\n",
      "\n",
      "    accuracy                          0.606       180\n",
      "   macro avg      0.606     0.606     0.606       180\n",
      "weighted avg      0.606     0.606     0.606       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.952 | Valid loss:   0.697 | Valid F1:   0.500\n",
      "Epoch:2 | Train Loss: 0.196 | Valid loss:   0.696 | Valid F1:   0.516\n",
      "Epoch:3 | Train Loss: 0.085 | Valid loss:   0.695 | Valid F1:   0.550\n",
      "Epoch:6 | Train Loss: 0.034 | Valid loss:   0.696 | Valid F1:   0.555\n",
      "Epoch:10 | Train Loss: 0.014 | Valid loss:   0.695 | Valid F1:   0.561\n",
      "Epoch:16 | Train Loss: 0.005 | Valid loss:   0.698 | Valid F1:   0.572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.671     0.633     0.651        90\n",
      "           1      0.653     0.689     0.670        90\n",
      "\n",
      "    accuracy                          0.661       180\n",
      "   macro avg      0.662     0.661     0.661       180\n",
      "weighted avg      0.662     0.661     0.661       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=32, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.737 | Valid loss:   0.677 | Valid F1:   0.578\n",
      "Epoch:3 | Train Loss: 0.048 | Valid loss:   0.675 | Valid F1:   0.583\n",
      "Epoch:4 | Train Loss: 0.031 | Valid loss:   0.674 | Valid F1:   0.583\n",
      "Epoch:5 | Train Loss: 0.023 | Valid loss:   0.674 | Valid F1:   0.589\n",
      "Epoch:8 | Train Loss: 0.011 | Valid loss:   0.674 | Valid F1:   0.600\n",
      "Epoch:9 | Train Loss: 0.009 | Valid loss:   0.674 | Valid F1:   0.605\n",
      "Epoch:11 | Train Loss: 0.006 | Valid loss:   0.673 | Valid F1:   0.611\n",
      "Epoch:13 | Train Loss: 0.004 | Valid loss:   0.673 | Valid F1:   0.617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.619     0.578     0.598        90\n",
      "           1      0.604     0.644     0.624        90\n",
      "\n",
      "    accuracy                          0.611       180\n",
      "   macro avg      0.612     0.611     0.611       180\n",
      "weighted avg      0.612     0.611     0.611       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.593 | Valid loss:   0.690 | Valid F1:   0.555\n",
      "Epoch:35 | Train Loss: 0.002 | Valid loss:   0.707 | Valid F1:   0.556\n",
      "Epoch:498 | Train Loss: 0.001 | Valid loss:   0.709 | Valid F1:   0.561\n",
      "Epoch:499 | Train Loss: 0.001 | Valid loss:   0.710 | Valid F1:   0.567\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.616     0.589     0.602        90\n",
      "           1      0.606     0.633     0.620        90\n",
      "\n",
      "    accuracy                          0.611       180\n",
      "   macro avg      0.611     0.611     0.611       180\n",
      "weighted avg      0.611     0.611     0.611       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=64, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.814 | Valid loss:   0.687 | Valid F1:   0.576\n",
      "Epoch:2 | Train Loss: 0.112 | Valid loss:   0.683 | Valid F1:   0.580\n",
      "Epoch:3 | Train Loss: 0.053 | Valid loss:   0.683 | Valid F1:   0.586\n",
      "Epoch:6 | Train Loss: 0.023 | Valid loss:   0.680 | Valid F1:   0.587\n",
      "Epoch:9 | Train Loss: 0.014 | Valid loss:   0.680 | Valid F1:   0.592\n",
      "Epoch:13 | Train Loss: 0.009 | Valid loss:   0.680 | Valid F1:   0.593\n",
      "Epoch:16 | Train Loss: 0.006 | Valid loss:   0.680 | Valid F1:   0.599\n",
      "Epoch:21 | Train Loss: 0.004 | Valid loss:   0.680 | Valid F1:   0.604\n",
      "Epoch:24 | Train Loss: 0.003 | Valid loss:   0.680 | Valid F1:   0.605\n",
      "Epoch:26 | Train Loss: 0.003 | Valid loss:   0.680 | Valid F1:   0.605\n",
      "Epoch:30 | Train Loss: 0.002 | Valid loss:   0.680 | Valid F1:   0.605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.558     0.589     0.573        90\n",
      "           1      0.565     0.533     0.549        90\n",
      "\n",
      "    accuracy                          0.561       180\n",
      "   macro avg      0.561     0.561     0.561       180\n",
      "weighted avg      0.561     0.561     0.561       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.687 | Valid loss:   0.710 | Valid F1:   0.520\n",
      "Epoch:4 | Train Loss: 0.032 | Valid loss:   0.713 | Valid F1:   0.522\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.644     0.622     0.633        90\n",
      "           1      0.634     0.656     0.645        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=64, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.556 | Valid loss:   0.676 | Valid F1:   0.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.670     0.656     0.663        90\n",
      "           1      0.663     0.678     0.670        90\n",
      "\n",
      "    accuracy                          0.667       180\n",
      "   macro avg      0.667     0.667     0.667       180\n",
      "weighted avg      0.667     0.667     0.667       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.662 | Valid loss:   0.694 | Valid F1:   0.489\n",
      "Epoch:2 | Train Loss: 0.125 | Valid loss:   0.698 | Valid F1:   0.511\n",
      "Epoch:4 | Train Loss: 0.056 | Valid loss:   0.701 | Valid F1:   0.511\n",
      "Epoch:499 | Train Loss: 0.002 | Valid loss:   0.720 | Valid F1:   0.516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.614     0.600     0.607        90\n",
      "           1      0.609     0.622     0.615        90\n",
      "\n",
      "    accuracy                          0.611       180\n",
      "   macro avg      0.611     0.611     0.611       180\n",
      "weighted avg      0.611     0.611     0.611       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=128, shared_emb_size=2048, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.622 | Valid loss:   0.690 | Valid F1:   0.554\n",
      "Epoch:2 | Train Loss: 0.107 | Valid loss:   0.691 | Valid F1:   0.573\n",
      "Epoch:3 | Train Loss: 0.065 | Valid loss:   0.690 | Valid F1:   0.579\n",
      "Epoch:6 | Train Loss: 0.031 | Valid loss:   0.689 | Valid F1:   0.586\n",
      "Epoch:26 | Train Loss: 0.005 | Valid loss:   0.691 | Valid F1:   0.588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.604     0.611     0.608        90\n",
      "           1      0.607     0.600     0.603        90\n",
      "\n",
      "    accuracy                          0.606       180\n",
      "   macro avg      0.606     0.606     0.606       180\n",
      "weighted avg      0.606     0.606     0.606       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=1024\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.651 | Valid loss:   0.691 | Valid F1:   0.499\n",
      "Epoch:2 | Train Loss: 0.182 | Valid loss:   0.689 | Valid F1:   0.527\n",
      "Epoch:3 | Train Loss: 0.113 | Valid loss:   0.688 | Valid F1:   0.543\n",
      "Epoch:4 | Train Loss: 0.082 | Valid loss:   0.689 | Valid F1:   0.544\n",
      "Epoch:5 | Train Loss: 0.064 | Valid loss:   0.689 | Valid F1:   0.556\n",
      "Epoch:6 | Train Loss: 0.051 | Valid loss:   0.689 | Valid F1:   0.566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.630     0.644     0.637        90\n",
      "           1      0.636     0.622     0.629        90\n",
      "\n",
      "    accuracy                          0.633       180\n",
      "   macro avg      0.633     0.633     0.633       180\n",
      "weighted avg      0.633     0.633     0.633       180\n",
      "\n",
      "Training with dropout=0.4, lr=0.0001, batch_size=128, shared_emb_size=1024, proj_emb_size=256\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.683 | Valid loss:   0.692 | Valid F1:   0.542\n",
      "Epoch:3 | Train Loss: 0.104 | Valid loss:   0.697 | Valid F1:   0.548\n",
      "Epoch:5 | Train Loss: 0.056 | Valid loss:   0.699 | Valid F1:   0.549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.637     0.644     0.641        90\n",
      "           1      0.640     0.633     0.637        90\n",
      "\n",
      "    accuracy                          0.639       180\n",
      "   macro avg      0.639     0.639     0.639       180\n",
      "weighted avg      0.639     0.639     0.639       180\n",
      "\n",
      "Best hyper-parameters: (0.3, 0.001, 64, 1024, 256) with loss: 0.6113027930259705 and F1 score of 0.6998517786561265\n"
     ]
    }
   ],
   "source": [
    "speaker = \"Y\"  # or \"Y\" for Speaker Dependent else \"n\" or \"N\"\n",
    "mode = \"VTA\"  # \"V\" for Video, \"T\" for Text, \"A\" for Audio respectively\n",
    "context = \"Y\"  # \"y\" or \"Y\" for Context Dependent else \"n\" or \"N\"\n",
    "\n",
    "MODEL_NAME, parameters, COMMAND = get_model_and_parameters(mode, speaker, context)\n",
    "\n",
    "\n",
    "# Define ranges for hyper-parameters\n",
    "dropout_values = [0.2, 0.3, 0.4]\n",
    "learning_rate_values = [0.001, 0.0001]\n",
    "batch_size_values = [32, 64, 128]\n",
    "shared_emb_values = [2048, 1024]\n",
    "proj_emb_values = [1024, 256]\n",
    "\n",
    "# Function to initialize and train the model, change model according to ablation settings\n",
    "def train_and_evaluate_model(dropout, lr, batch_size, shared_emb_size, proj_emb_size):\n",
    "    seed()\n",
    "    train_loader = DataLoader(CD1, batch_size=batch_size, num_workers=0, pin_memory=False, worker_init_fn=seed_worker)\n",
    "    seed()\n",
    "    val_loader = DataLoader(CD2, batch_size=batch_size, num_workers=0, pin_memory=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    seed()\n",
    "    mod = Speaker_Dependent_Triple_Mode_with_Context(num_classes=2, dropout=dropout, shared_embedding=shared_emb_size, projection_embedding=proj_emb_size)\n",
    "    \n",
    "    mod.to(device)\n",
    "\n",
    "    seed()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    "    \n",
    "    seed()\n",
    "    optimizer = optim.Adam(params=mod.parameters(), betas=(0.5, 0.99), lr=lr)\n",
    "    \n",
    "    (true, pred), epo = training(\n",
    "        mod=mod, \n",
    "        criterion=criterion, \n",
    "        optimizer=optimizer, \n",
    "        call=COMMAND, \n",
    "        train_loader=train_loader, \n",
    "        valid_loader=val_loader, \n",
    "        fold=0, \n",
    "        e=epoch, \n",
    "        patience=patience\n",
    "    )\n",
    "    test_loader = DataLoader(CD3, batch_size, num_workers=0, pin_memory=False, worker_init_fn=seed_worker)\n",
    "    test_accuracy, test_loss = evaluation(test_loader, mod, call=COMMAND, report=True)\n",
    "    return test_accuracy, test_loss\n",
    "    \n",
    "\n",
    "# Grid search\n",
    "best_params = None\n",
    "best_accuracy = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "\n",
    "for dropout, lr, batch_size, shared_emb_size, proj_emb_size in itertools.product(\n",
    "    dropout_values, learning_rate_values, batch_size_values, shared_emb_values, proj_emb_values\n",
    "):\n",
    "    print(f'Training with dropout={dropout}, lr={lr}, batch_size={batch_size}, shared_emb_size={shared_emb_size}, proj_emb_size={proj_emb_size}')\n",
    "    accuracy, loss = train_and_evaluate_model(dropout, lr, batch_size, shared_emb_size, proj_emb_size)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        best_accuracy = accuracy\n",
    "        best_params = (dropout, lr, batch_size, shared_emb_size, proj_emb_size)\n",
    "        print(f'New best params: {best_params} with loss: {best_loss} and F1 score of :{best_accuracy}')\n",
    "\n",
    "print(f'Best hyper-parameters: {best_params} with loss: {best_loss} and F1 score of {best_accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
