{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chwu/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/chwu/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import itertools\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset, random_split\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#We load in the previously extracted and saved features (last four vs final)\n",
    "import pickle\n",
    "with open('/home/chwu/nonlayered_mustard_updated_text.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.dropna(subset=['text_embeddings']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.rename(columns={\n",
    "    'text_embeddings': 'uText',\n",
    "    'audio_embeddings': 'uAudio',\n",
    "    'keyframe_embeddings': 'uVideo',\n",
    "    'Sarcasm':'SAR'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['cAudio'] = None\n",
    "\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cAudio'] = new_df.at[i, 'uAudio']\n",
    "\n",
    "new_df['cVideo'] = None\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cVideo'] = new_df.at[i, 'uVideo']\n",
    "\n",
    "new_df['cText'] = None\n",
    "\n",
    "for i in range(0, len(new_df) - 1, 2):\n",
    "    new_df.at[i + 1, 'cText'] = new_df.at[i, 'uText']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.dropna(subset=['cAudio']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, split1 = train_test_split(\n",
    "    new_df, test_size=0.2, stratify=new_df['SAR'], random_state=42)\n",
    "split23, split45 = train_test_split(\n",
    "    train, test_size=0.5, stratify=train['SAR'], random_state=42)\n",
    "split2, split3 = train_test_split(\n",
    "    split23, test_size=0.5, stratify=split23['SAR'], random_state=42)\n",
    "split4, split5 = train_test_split(\n",
    "    split45, test_size=0.5, stratify=split45['SAR'], random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reset index to the folds of data and save them to pickle for further experiments\n",
    "split1.reset_index(drop=True, inplace=True)\n",
    "split2.reset_index(drop=True, inplace=True)\n",
    "split3.reset_index(drop=True, inplace=True)\n",
    "split4.reset_index(drop=True, inplace=True)\n",
    "split5.reset_index(drop=True, inplace=True)\n",
    "\n",
    "split1.to_pickle('split1.pkl')\n",
    "split2.to_pickle('split2.pkl')\n",
    "split3.to_pickle('split3.pkl')\n",
    "split4.to_pickle('split4.pkl')\n",
    "split5.to_pickle('split5.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for loading in previously saved folds\n",
    "\n",
    "with open('/home/chwu/local-dir/fold1.pkl', 'rb') as f:\n",
    "    split1 = pickle.load(f)\n",
    "with open('/home/chwu/local-dir/fold2.pkl', 'rb') as f:\n",
    "    split2 = pickle.load(f)\n",
    "with open('/home/chwu/local-dir/fold3.pkl', 'rb') as f:\n",
    "    split3 = pickle.load(f)\n",
    "with open('/home/chwu/local-dir/fold4.pkl', 'rb') as f:\n",
    "    split4 = pickle.load(f)\n",
    "with open('/home/chwu/local-dir/fold5.pkl', 'rb') as f:\n",
    "    split5 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We map the five folds of data into appropriate formats so the Custom ContentDataset class can process them properly.\n",
    "\n",
    "dataset1 = {}\n",
    "\n",
    "dataset1 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in split1.iterrows()\n",
    "}\n",
    "\n",
    "dataset2 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in split2.iterrows()\n",
    "}\n",
    "\n",
    "dataset3 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in split3.iterrows()\n",
    "}\n",
    "\n",
    "dataset4 = {}\n",
    "\n",
    "dataset4 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in split4.iterrows()\n",
    "}\n",
    "\n",
    "dataset5 = {}\n",
    "\n",
    "dataset5 = {\n",
    "    row['SCENE']: {\n",
    "        'uText': row['uText'],\n",
    "        'cText': row['cText'],\n",
    "        'uAudio': row['uAudio'],\n",
    "        'cAudio': row['cAudio'],\n",
    "        'uVideo': row['uVideo'],\n",
    "        'cVideo': row['cVideo']\n",
    "    }\n",
    "    for _, row in split5.iterrows()\n",
    "}\n",
    "\n",
    "map1 = split1[['SCENE','SAR','SPEAKER']]\n",
    "map2 = split2[['SCENE','SAR','SPEAKER']]\n",
    "map3 = split3[['SCENE','SAR','SPEAKER']]\n",
    "map4 = split4[['SCENE','SAR','SPEAKER']]\n",
    "map5 = split5[['SCENE','SAR','SPEAKER']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We extract the full speaker list from the unsplit dataset\n",
    "speaker_list = sorted(list(df.SPEAKER.value_counts().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the attention function to reduce sequence dimensionality. irrelevant if using mean of last four representations.\n",
    "def apply_attention(tensor):\n",
    "    # Compute attention scores\n",
    "        attention_layer = torch.nn.Linear(768, 1)\n",
    "        attention_scores = attention_layer(tensor)  # [batch_size, seq_len, 1]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len, 1]\n",
    "    \n",
    "    # Weighted sum of the input tensor\n",
    "        attended_tensor = (tensor * attention_weights).sum(dim=-2)  # [batch_size, hidden_dim]\n",
    "    \n",
    "        return attended_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the custom Content Dataset class used to load in extracted features, speaker info, and sarcasm label.\n",
    "#this is adapted from MUStARD++'s code\n",
    "#commented out section is the version used for mean of last four layer\n",
    "class ContentDataset(Dataset):\n",
    "\n",
    "    def __init__(self, mapping, dataset, speaker_list):\n",
    "        self.mapping = mapping\n",
    "        self.dataset = dataset\n",
    "        self.speakers_mapping = speaker_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mapping)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        index = self.mapping.loc[idx, 'SCENE']\n",
    "        data = self.dataset[index]\n",
    "        label = int(self.mapping.loc[idx, 'SAR'])\n",
    "        spkr = np.eye(len(self.speakers_mapping))[self.speakers_mapping.index(\n",
    "            self.mapping.loc[idx, 'SPEAKER'])]\n",
    "        uText = data['uText'].squeeze()\n",
    "        cText = data['cText'].squeeze()\n",
    "        uAudio = apply_attention(data['uAudio']).squeeze()\n",
    "        cAudio = apply_attention(data['cAudio']).squeeze()\n",
    "        uVideo = apply_attention(data['uVideo']).squeeze()\n",
    "        cVideo = apply_attention(data['cVideo']).squeeze()\n",
    "\n",
    "        return uText, cText, uAudio, cAudio, uVideo, cVideo, spkr, label\n",
    "\n",
    "# class ContentDataset(Dataset):\n",
    "\n",
    "#     def __init__(self, mapping, dataset, speaker_list):\n",
    "#         self.mapping = mapping\n",
    "#         self.dataset = dataset\n",
    "#         self.speakers_mapping = speaker_list\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.mapping)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if torch.is_tensor(idx):\n",
    "#             idx = idx.tolist()\n",
    "\n",
    "#         index = self.mapping.loc[idx, 'SCENE']\n",
    "#         data = self.dataset[index]\n",
    "#         label = int(self.mapping.loc[idx, 'SAR'])\n",
    "#         spkr = np.eye(len(self.speakers_mapping))[self.speakers_mapping.index(\n",
    "#             self.mapping.loc[idx, 'SPEAKER'])]\n",
    "#         uText = data['uText'].squeeze()\n",
    "#         cText = data['cText'].squeeze()\n",
    "#         uAudio = data['uAudio'].squeeze()\n",
    "#         cAudio = data['cAudio'].squeeze()\n",
    "#         uVideo = data['uVideo'].squeeze()\n",
    "#         cVideo = data['cVideo'].squeeze()\n",
    "\n",
    "#         return uText, cText, uAudio, cAudio, uVideo, cVideo, spkr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 3, Column 'uText': Shape torch.Size([1, 11, 768])\n",
      "Row 3, Column 'uAudio': Shape torch.Size([1, 84, 768])\n",
      "Row 3, Column 'uVideo': Shape torch.Size([1, 197, 768])\n",
      "Row 3, Column 'cText': Shape torch.Size([1, 58, 768])\n",
      "Row 3, Column 'cAudio': Shape torch.Size([1, 1032, 768])\n",
      "Row 3, Column 'cVideo': Shape torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "#we first check the shape of the tensors in the dataframes\n",
    "for column in ['uText', 'uAudio', 'uVideo','cText','cAudio','cVideo']:\n",
    "    tensor_in_col = split2.loc[79, column]\n",
    "    print(f\"Row {3}, Column '{column}': Shape {tensor_in_col.shape}\")\n",
    "\n",
    "#we load the features from the datafram into the ContentDataset class with its respective fold\n",
    "CD1 = ContentDataset(mapping=map1, dataset=dataset1,speaker_list=speaker_list)\n",
    "CD2 = ContentDataset(mapping=map2, dataset=dataset2,speaker_list=speaker_list)\n",
    "CD3 = ContentDataset(mapping=map3, dataset=dataset3,speaker_list=speaker_list)\n",
    "CD4 = ContentDataset(mapping=map4, dataset=dataset4,speaker_list=speaker_list)\n",
    "CD5 = ContentDataset(mapping=map5, dataset=dataset5,speaker_list=speaker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uText: torch.Size([768])\n",
      "cText: torch.Size([768])\n",
      "uAudio: torch.Size([768])\n",
      "cAudio: torch.Size([768])\n",
      "uVideo: torch.Size([768])\n",
      "cVideo: torch.Size([768])\n",
      "spkr: (33,)\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "#we inspect whether a sample of the ContentDataset class is in the right shape\n",
    "index = 9 \n",
    "entry = CD2[index]\n",
    "\n",
    "# Print the entry\n",
    "print(\"uText:\", entry[0].shape)\n",
    "print(\"cText:\", entry[1].shape)\n",
    "print(\"uAudio:\", entry[2].shape)\n",
    "print(\"cAudio:\", entry[3].shape)\n",
    "print(\"uVideo:\", entry[4].shape)\n",
    "print(\"cVideo:\", entry[5].shape)\n",
    "print(\"spkr:\", entry[6].shape)\n",
    "print(\"label:\", entry[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code blocks (seed, evaluation, training, get_command, get_models_and_parameters, seed_worker) is largely replicated from MUStARD++'s training and evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed():\n",
    "    \"\"\" This method is used for seeding the code and different points\"\"\"\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(loader, mod, call, report=False, flag=False):\n",
    "    \"\"\"Args:\n",
    "            loader:\n",
    "                It is the validation dataloader\n",
    "            mod:\n",
    "                It is the best model, which we have to evaluate\n",
    "            call:\n",
    "                call is the COMMAND to be excuted to run the forward method of the model\n",
    "                it changed as per the modality and other possible input\n",
    "            report:\n",
    "                If True then the classification report for the validation set is printed\n",
    "            flag:\n",
    "                if True the instead of evaluation metrics, method returns the class labels (predictions)\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = []\n",
    "        true = []\n",
    "        total_loss = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion.to(device)\n",
    "        seed()\n",
    "        for batch in loader:\n",
    "            uText = batch[0].float().to(device)\n",
    "            cText = batch[1].float().to(device)\n",
    "            uAudio = batch[2].float().to(device)\n",
    "            cAudio = batch[3].float().to(device)\n",
    "            uVideo = batch[4].float().to(device)\n",
    "            cVideo = batch[5].float().to(device)\n",
    "            speaker = batch[6].float().to(device)\n",
    "            y_true = batch[7].long().to(device)\n",
    "            del batch\n",
    "            output = torch.softmax(eval(call), dim=1)\n",
    "            loss = criterion(output, y_true)\n",
    "            del uText, cText, uAudio, cAudio, uVideo, cVideo, speaker\n",
    "            # with torch.cuda.device(device):\n",
    "            #     torch.cuda.empty_cache()\n",
    "            total_loss.append(loss)\n",
    "            pred.extend(output.detach().cpu().tolist())\n",
    "            true.extend(y_true.tolist())\n",
    "        if flag:\n",
    "            return true, np.argmax(pred, axis=1)\n",
    "        if report:\n",
    "            print(classification_report(true, np.argmax(pred, axis=1), digits=3))\n",
    "\n",
    "        \n",
    "        return f1_score(true, np.argmax(pred, axis=1), average='macro'), sum(total_loss)/len(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(mod, criterion, optimizer, call, train_loader, valid_loader, fold, e=500, patience=5, report=False,save=True):\n",
    "    \"\"\"Args:\n",
    "            mod :\n",
    "                It is the mod we have to train\n",
    "            criterion :\n",
    "                Loss function, here we have Cross entropy loss\n",
    "            optimizer :\n",
    "              object of torch.optim class\n",
    "            call:\n",
    "                call is the COMMAND to be excuted to run the forward method of the model\n",
    "                it changed as per the modality and other possible input\n",
    "            train_loader:\n",
    "                It is a instance of train dataloader\n",
    "            valid_loader:\n",
    "                It is a instance of validation dataloader, it is given as a input to evaluation class\n",
    "            fold:\n",
    "                5 FOLD {0,1,2,3,4}\n",
    "            e:\n",
    "                maximum epoch\n",
    "            patience:\n",
    "                how many epoch to wait after the early stopping condition in satisfied\n",
    "            report:\n",
    "                It True then the classification report for the validation set is printed, it is given as a input to evaluation class\n",
    "            save:\n",
    "                If true then best model for each fold is saved\n",
    "\n",
    "    \"\"\"\n",
    "    print('-'*100)\n",
    "    train_losses = [0]\n",
    "    valid_losses = [0]\n",
    "    max_f1 = 0\n",
    "    patience_flag = 1\n",
    "    best_epoch = 0\n",
    "    print(fold, e, patience)\n",
    "\n",
    "    while e > 0:\n",
    "        total_loss = []\n",
    "        seed()\n",
    "        for batch_data in train_loader:\n",
    "            uText = batch_data[0].float().to(device)\n",
    "            cText = batch_data[1].float().to(device)\n",
    "            uAudio = batch_data[2].float().to(device)\n",
    "            cAudio = batch_data[3].float().to(device)\n",
    "            uVideo = batch_data[4].float().to(device)\n",
    "            cVideo = batch_data[5].float().to(device)\n",
    "            speaker = batch_data[6].float().to(device)\n",
    "            y_true = batch_data[7].long().to(device)\n",
    "            del batch_data\n",
    "            output = eval(call)\n",
    "            loss = criterion(output, y_true)\n",
    "            del uText, cText, uAudio, cAudio, uVideo, cVideo, speaker\n",
    "            # with torch.cuda.device(device):\n",
    "            #     torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.append(loss.detach().item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            valid_f1, valid_loss = evaluation(\n",
    "                valid_loader, mod, call, report, False)\n",
    "            train_losses.append(sum(total_loss)/len(total_loss))\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            e = e-1\n",
    "            if max_f1 < valid_f1:\n",
    "                max_f1 = valid_f1\n",
    "                best_model = mod\n",
    "                best_epoch = 500-e\n",
    "                print(\n",
    "                    f'Epoch:{best_epoch} | Train Loss: {loss.detach().item():.3f} | Valid loss: { valid_loss.detach().item():7.3f} | Valid F1: { valid_f1:7.3f}')\n",
    "\n",
    "            if abs(train_losses[-2]-train_losses[-1]) < 0.0001:\n",
    "                if patience_flag == 1:\n",
    "                    e = patience\n",
    "                    patience_flag = 0\n",
    "            else:\n",
    "                patience_flag = 1\n",
    "\n",
    "    # if save:\n",
    "    #     best_model.to(device)\n",
    "    #     torch.save(best_model.state_dict(), 'MPP_Code/saved_models/sarc/' +\n",
    "    #                         filename+'_'+str(fold)+'.pth')\n",
    "                \n",
    "    return evaluation(valid_loader, best_model, call, report, True), best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_command(input_modes, context_flag, speaker_flag):\n",
    "    \"\"\"\n",
    "        This method is used to create the COMMAND to execute the forward methof of particular model,\n",
    "        Depending upon the input combination\n",
    "        Args:\n",
    "            input_modes:\n",
    "                Input Modality {VTA, VT, VA, TA, V, T, A}\n",
    "            context_flag :\n",
    "                If true then \"with context\" else \"without context\" \n",
    "            speaker_flag:\n",
    "                if true then Speaker dependent else Speaker INdependent\n",
    "    \"\"\"\n",
    "    if input_modes == 'VTA':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uText, 'uC':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cText, 'cC':cAudio\"\n",
    "\n",
    "    elif input_modes == 'VT':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uText\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cText\"\n",
    "\n",
    "    elif input_modes == 'VA':\n",
    "        COMMAND = \"mod(**{'uA':uVideo, 'uB':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo, 'cB':cAudio\"\n",
    "\n",
    "    elif input_modes == 'TA':\n",
    "        COMMAND = \"mod(**{'uA':uText, 'uB':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cText, 'cB':cAudio\"\n",
    "\n",
    "    elif input_modes == 'T':\n",
    "        COMMAND = \"mod(**{'uA':uText\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cText\"\n",
    "\n",
    "    elif input_modes == 'V':\n",
    "        COMMAND = \"mod(**{'uA':uVideo\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cVideo\"\n",
    "\n",
    "    elif input_modes == 'A':\n",
    "        COMMAND = \"mod(**{'uA':uAudio\"\n",
    "        if context_flag == 'y':\n",
    "            COMMAND += \",'cA':cAudio\"\n",
    "    if speaker_flag == 'y':\n",
    "        COMMAND += \",'speaker_embedding':speaker})\"\n",
    "    else:\n",
    "        COMMAND += \"})\"\n",
    "\n",
    "    return COMMAND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_classes has been changed to 2 as we are performing binary sarcasm detection\n",
    "#all tensors shape have been changed to 768 to correspond to the tensor shape of Data2Vec after reshaping with ContentDataset\n",
    "\n",
    "def get_model_and_parameters(mode, speaker, context):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - mode: A string representing the mode.\n",
    "    - speaker: A string ('y' or 'n') indicating whether the speaker is dependent or not.\n",
    "    - context: A string ('y' or 'n') indicating whether context is considered or not.\n",
    "    \"\"\"\n",
    "    # Here we are sorting VTA in descending order, in order to have consistency in the model\n",
    "    input_modes = ''.join(reversed(sorted(list(mode.upper()))))\n",
    "\n",
    "    parameters = {}\n",
    "    MODEL_NAME = 'Speaker_'\n",
    "\n",
    "    parameters['num_classes'] = 2\n",
    "\n",
    "    if speaker.lower() == 'y':\n",
    "        MODEL_NAME += 'Dependent_'\n",
    "        parameters['n_speaker'] = len(speaker_list)\n",
    "    else:\n",
    "        MODEL_NAME += 'Independent_'\n",
    "\n",
    "    if len(input_modes) == 3:\n",
    "        MODEL_NAME += 'Triple_'\n",
    "        parameters['input_embedding_A'] = 768\n",
    "        parameters['input_embedding_B'] = 768\n",
    "        parameters['input_embedding_C'] = 768\n",
    "\n",
    "    elif len(input_modes) == 2:\n",
    "        MODEL_NAME += 'Dual_'\n",
    "        parameters['input_embedding_A'] = 768\n",
    "        parameters['input_embedding_B'] = 768 \n",
    "    else:\n",
    "        MODEL_NAME += 'Single_'\n",
    "        parameters['input_embedding_A'] = 768 \n",
    "\n",
    "    MODEL_NAME += 'Mode_with'\n",
    "    MODEL_NAME += 'out' if context.lower() == 'n' else ''\n",
    "    MODEL_NAME += '_Context'\n",
    "\n",
    "    MODEL_NAME = 'emotion_classification_model.' + MODEL_NAME\n",
    "\n",
    "    COMMAND = get_command(input_modes, context.lower(), speaker.lower())  # Ensure get_command is defined\n",
    "    return MODEL_NAME, parameters, COMMAND\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    \"\"\" This method is used for seeding the worker in the dataloader\"\"\"\n",
    "    worker_seed = 42\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a single ablation configuration of model. All models are identical to sarcasm detection model of MUStARD++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Speaker_Dependent_Triple_Mode_with_Context(nn.Module):\n",
    "    def __init__(self, n_speaker=len(speaker_list), input_embedding_A=768, input_embedding_B=768, input_embedding_C=768, shared_embedding=0, projection_embedding=0, dropout=0, num_classes=2):\n",
    "        super(Speaker_Dependent_Triple_Mode_with_Context, self).__init__()\n",
    "\n",
    "        self.n_speaker = n_speaker\n",
    "\n",
    "        self.input_embedding_A = input_embedding_A\n",
    "        self.input_embedding_B = input_embedding_B\n",
    "        self.input_embedding_C = input_embedding_C\n",
    "\n",
    "        self.shared_embedding = shared_embedding\n",
    "        self.projection_embedding = projection_embedding\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.A_context_share = nn.Linear(self.input_embedding_A, self.shared_embedding)\n",
    "        self.A_utterance_share = nn.Linear(self.input_embedding_A, self.shared_embedding)\n",
    "\n",
    "        self.C_context_share = nn.Linear(self.input_embedding_C, self.shared_embedding)\n",
    "        self.C_utterance_share = nn.Linear(self.input_embedding_C, self.shared_embedding)\n",
    "\n",
    "        self.B_context_share = nn.Linear(self.input_embedding_B, self.shared_embedding)\n",
    "        self.B_utterance_share = nn.Linear(self.input_embedding_B, self.shared_embedding)\n",
    "\n",
    "        self.norm_A_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_A_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.norm_C_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_C_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.norm_B_context = nn.BatchNorm1d(self.shared_embedding)\n",
    "        self.norm_B_utterance = nn.BatchNorm1d(self.shared_embedding)\n",
    "\n",
    "        self.collaborative_gate_1 = nn.Linear(2 * self.shared_embedding, self.projection_embedding)\n",
    "        self.collaborative_gate_2 = nn.Linear(self.projection_embedding, self.shared_embedding)\n",
    "\n",
    "        self.pred_module = nn.Sequential(\n",
    "            nn.Linear(self.n_speaker + 3 * self.shared_embedding, 2 * self.shared_embedding),\n",
    "            nn.BatchNorm1d(2 * self.shared_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2 * self.shared_embedding, self.shared_embedding),\n",
    "            nn.BatchNorm1d(self.shared_embedding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.shared_embedding, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        )\n",
    "\n",
    "    def attention(self, featureA, featureB):\n",
    "        \"\"\" This method takes two features and calculates the attention \"\"\"\n",
    "        input = torch.cat((featureA, featureB), dim=1)\n",
    "        return nn.functional.softmax(self.collaborative_gate_1(input), dim=1)\n",
    "    \n",
    "\n",
    "    def attention_aggregator(self, feA, feB, feC, feD, feE, feF):\n",
    "        \"\"\" This method calculates the attention for feA with respect to others\"\"\"\n",
    "        input = self.attention(feA, feB) + self.attention(feA, feC) + self.attention(feA, feD) + self.attention(feA, feE) + self.attention(feA, feF)\n",
    "        return nn.functional.softmax(self.collaborative_gate_2(input), dim=1)\n",
    "\n",
    "    def forward(self, uA, cA, uB, cB, uC, cC, speaker_embedding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            uA: Utterance Video\n",
    "            uB: Utterance Text\n",
    "            uC: Utterance Audio\n",
    "            cA: Context Video\n",
    "            cB: Context Text\n",
    "            cC: Context Audio\n",
    "\n",
    "        Returns:\n",
    "            probability of emotion classes\n",
    "        \"\"\"\n",
    "        # Pooling or averaging the sequences to match the expected input dimensions for linear layers\n",
    "        \n",
    "        shared_A_context = self.norm_A_context(nn.functional.relu(self.A_context_share(cA)))\n",
    "        shared_A_utterance = self.norm_A_utterance(nn.functional.relu(self.A_utterance_share(uA)))\n",
    "\n",
    "        shared_C_context = self.norm_C_context(nn.functional.relu(self.C_context_share(cC)))\n",
    "        shared_C_utterance = self.norm_C_utterance(nn.functional.relu(self.C_utterance_share(uC)))\n",
    "\n",
    "        shared_B_context = self.norm_B_context(nn.functional.relu(self.B_context_share(cB)))\n",
    "        shared_B_utterance = self.norm_B_utterance(nn.functional.relu(self.B_utterance_share(uB)))\n",
    "\n",
    "        updated_shared_A = shared_A_utterance * self.attention_aggregator(\n",
    "            shared_A_utterance, shared_A_context, shared_C_context, shared_C_utterance, shared_B_context, shared_B_utterance)\n",
    "        updated_shared_C = shared_C_utterance * self.attention_aggregator(\n",
    "            shared_C_utterance, shared_C_context, shared_A_context, shared_A_utterance, shared_B_context, shared_B_utterance)\n",
    "        updated_shared_B = shared_B_utterance * self.attention_aggregator(\n",
    "            shared_B_utterance, shared_B_context, shared_A_context, shared_A_utterance, shared_C_context, shared_C_utterance)\n",
    "\n",
    "        temp = torch.cat((updated_shared_A, updated_shared_C), dim=1)\n",
    "        input = torch.cat((temp, updated_shared_B), dim=1)\n",
    "\n",
    "        input = torch.cat((input, speaker_embedding), dim=1)\n",
    "\n",
    "        return self.pred_module(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "epoch = 500  # Number of epochs\n",
    "patience = 5  # Patience\n",
    "gpu = 0  # Which GPU to use\n",
    "seed_variable = 42  # SEED value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 240, 241, 241)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CD1),len(CD2),len(CD3),len(CD4),len(CD5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0 500 5\n",
      "Epoch:1 | Train Loss: 0.651 | Valid loss:   0.615 | Valid F1:   0.674\n",
      "Epoch:2 | Train Loss: 0.527 | Valid loss:   0.591 | Valid F1:   0.723\n",
      "Epoch:3 | Train Loss: 0.407 | Valid loss:   0.564 | Valid F1:   0.757\n",
      "Epoch:4 | Train Loss: 0.231 | Valid loss:   0.544 | Valid F1:   0.779\n",
      "Epoch:7 | Train Loss: 0.094 | Valid loss:   0.523 | Valid F1:   0.796\n",
      "Epoch:10 | Train Loss: 0.022 | Valid loss:   0.498 | Valid F1:   0.817\n",
      "Epoch:11 | Train Loss: 0.018 | Valid loss:   0.487 | Valid F1:   0.825\n",
      "Epoch:12 | Train Loss: 0.006 | Valid loss:   0.477 | Valid F1:   0.850\n",
      "Epoch:13 | Train Loss: 0.005 | Valid loss:   0.465 | Valid F1:   0.858\n",
      "Epoch:14 | Train Loss: 0.004 | Valid loss:   0.460 | Valid F1:   0.862\n",
      "Epoch:15 | Train Loss: 0.003 | Valid loss:   0.458 | Valid F1:   0.867\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.598     0.579     0.588       121\n",
      "           1      0.589     0.608     0.598       120\n",
      "\n",
      "    accuracy                          0.593       241\n",
      "   macro avg      0.594     0.593     0.593       241\n",
      "weighted avg      0.594     0.593     0.593       241\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here we train and evaluate models for a single fold. \n",
    "\n",
    "speaker = \"Y\"  # or \"Y\" for Speaker Dependent else \"n\" or \"N\"\n",
    "mode = \"VTA\"  # \"V\" for Video, \"T\" for Text, \"A\" for Audio respectively\n",
    "context = \"Y\"  # \"y\" or \"Y\" for Context Dependent else \"n\" or \"N\"\n",
    "\n",
    "#change depending on best hyperparameters for each ablation configuration\n",
    "dropout=0.2\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "shared_emb_size = 2048\n",
    "proj_emb_size = 256\n",
    "\n",
    "MODEL_NAME, parameters, COMMAND = get_model_and_parameters(mode, speaker, context)\n",
    "\n",
    "#modify this based on each fold testing currently, selecting 200 samples as validation set\n",
    "train_CD = ConcatDataset([CD1, CD2, CD3, CD4])\n",
    "split_lengths = [200, 1002]\n",
    "val_CD, subset2 = random_split(train_CD, split_lengths)\n",
    "\n",
    "seed()\n",
    "train_loader = DataLoader(train_CD, batch_size=batch_size, num_workers=0, shuffle=True, pin_memory=False, worker_init_fn=seed_worker)\n",
    "seed()\n",
    "val_loader = DataLoader(val_CD, batch_size=batch_size, num_workers=0, shuffle=True, pin_memory=False, worker_init_fn=seed_worker)\n",
    "\n",
    "seed()\n",
    "mod = Speaker_Dependent_Triple_Mode_with_Context(n_speaker = len(speaker), num_classes=2, dropout=dropout, shared_embedding=shared_emb_size, projection_embedding=proj_emb_size)\n",
    "mod.to(device)\n",
    "\n",
    "seed()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "    \n",
    "seed()\n",
    "optimizer = optim.Adam(params=mod.parameters(), betas=(0.5, 0.99), lr=lr)\n",
    "    \n",
    "(true, pred), epo = training(\n",
    "    mod=mod, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    call=COMMAND, \n",
    "        train_loader=train_loader, \n",
    "        valid_loader=val_loader, \n",
    "        fold=0, \n",
    "        e=epoch, \n",
    "        patience=patience\n",
    "    )\n",
    "test_loader = DataLoader(CD5, batch_size, num_workers=0,shuffle=True, pin_memory=False, worker_init_fn=seed_worker)\n",
    "test_accuracy, test_loss = evaluation(test_loader, mod, call=COMMAND, report=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
